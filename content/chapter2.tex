% !TeX root = ../main-english.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% -*- coding:utf-8 mod:LaTeX -*-

%This smart spell only works if no changes have been made to the chapter
%using the options proposed in preambel/chapterheads.tex.
\setchapterpreamble[u]{%
	\dictum[Ian Holmes, in an \href{https://twitter.com/ianholmes/status/288689712636493824}{\#overlyhonestmethods tweet}]{You can download our code from the URL supplied. Good luck downloading the only	postdoc who can get it to run, though.}
}


\chapter{Ensuring computational reproducibility across computational environments}
\label{chap:k3}

% from NISO: Beyond their potential to mitigate transparency and reproducibility issues, these practices provide important benefits for individual researchers by increasing exposure, reputation, chances of publication, number of citations, media attention, potential collaborations, and position and funding opportunities (Allen and Mehler, 2019; McKiernan et al., 2016; Nosek et al., 2022; Markowetz, 2015; Hunt, 2019).
Partially fueled by external incentives or requirements \citep{mckiernan2016open, dfg}, research curricula founded within the Open Science Movement \citep{munafo2017manifesto, poldrack2017scanning}, and a growing ecosystem of openly available infrastructure and tools \citep{NISO2022119623}, practices of publishing reproducibly are becoming more frequent.
Widespread sharing of code and data allows researchers to verify, reuse, and improve upon past work \citep{borghi2018data}.
Grass-roots movements such as Reprohack (\href{https://www.reprohack.org/}{www.reprohack.org}) or the ``Ten Years Reproducibility Challenge'' (\href{https://rescience.github.io/ten-years/}{rescience.github.io/ten-years}) train researchers to check published studies for reproducibility.
Consequently, attempts to reproduce previous studies often happen in different computational environments than those that originally created the results in question.
Ensuring computational reproducibility across computational environments is, however, a difficult technical challenge.
This following chapter outlines first its challenges, particularly in the field of neuroimaging, then its opportunities, and lastly an implementation to ensure computational reproducibility across computational environments.

\section{The origins of reproducibility}

% from NISO: psychology (Open Science Collaboration, 2015; Klein et al., 2018), social sciences (Camerer et al., 2016, 2018), neuroimaging (Munafò et al., 2017; Botvinik-Nezer et al., 2020; Li et al., 2021), preclinical cancer biology research (Errington et al., 2021; Errington et al., 2021), and more (Hutson, 2018; Nissen et al., 2016; Serra-Garcia and Gneezy, 2021).
Over the past decade, interest in reproducibility has been fueled by salient failures to reconfirm published results -- often termed \textit{reproducibility crises} -- in numerous fields \citep{baker20161}, from psychology \citep{open2015estimating}, to biomedical imaging \citep{wagner202310}, to artificial intelligence \citep{hutson2018artificial}, or econonmics \citep{camerer2016evaluating}.
However, proposals to increase reproducibility, transparency, and robustness of science were made independently in various disciplines long before the current trend, in some cases dating back several centuries \citep[such as Boyle (1666), as cited in][]{RobertBoylesDesigneaboutNaturalHistory}.
Even the field of \textit{computational reproducibility} originated already more than 30 years ago in the field of seismology \citep{claerbout1992electronic, buckheit1995wavelab}, despite increased usage of the term in scientific literature only from 2015 onward (see \cref{fig:ngram}).
Consequently, the terminology around reproducibility has varied considerably over the years and across domains, and there is no universally agreed upon standardization of terminology in place yet \citep{barba2018terminologies}.
To disambiguate between several conflicting definitions of terms around reproducibility that are in active use, we shall define the terms used in this thesis as follows:

\subsubsection{Reproducibility}

Following the definition of \citet{peng2006}, \textit{reproducibility} refers to the practice of verifying a published result with the same methods and materials used by the original authors.

\subsubsection{Replicability}

\textit{Replicability}, on the other hand, refers to strengthening scientific evidence when several independent researchers find similar results using ``independent data, analytical methods, laboratories, and instruments'' \citep{peng2006}.

\subsubsection{Computational Reproducibility}

\textit{Computational reproducibility}, finally, matches the definition put forward in the 2019 report on ``Reproducibility and Replicability in Science'' by the \citet{engineering2019reproducibility}: ``We define reproducibility to mean computational reproducibility – obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis''.


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{google_ngram_reproducibility_2023-05-08.pdf}
	\caption[Computational reproducibility in the literature]{Popularity of computational reproducibility: A chart of the frequencies of the n-gram ``computational reproducibility" (using yearly count, normalized be the numbers of publications in each year) in literature included in the English(2019) corpus of Google books. This graph has been created using the Google Ngram Viewer (\href{https://books.google.com/ngrams/info}{books.google.com/ngrams}) \citep{michel2011quantitative}}
	\label{fig:ngram}
\end{figure}


\subsection{``Everything matters'' for computational reproducibility in neuroimaging}

% maybe NARPS paper?`

As this definition of computational reproducibility implies, the building blocks of research output extend to more than the files that constitute the actual research output, but also to all elements involved in its generation \citep{claerbout1992electronic}.
Consider different types of research output:
Raw data originates from acquisitions based on - potentially ongoing - experiments, raw data transformations, or data cleaning.
Processed data or results stem from computations with analysis code or software in specific versions on particular data.
And software, expressed in raw (code) or derived (transformed into executable) form, is created or used in specific computational environments, with compilers, underlying libraries, and systems in distinct versions.
Consequently, these building blocks play an integral part in the genesis of research outputs, and changes in these building blocks or their composition can translate to changes in the resulting research output.\\
Because of their complexity, neuroimaging studies face obstacles for reproducibility.
For one, the details of how code, software, or data have been used to generate a research output, such as analysis parameterization, the subset of data used as input, or sequence and invocation of employed software tools, are volatile.
Shared code and data does not always suffice to reproduce a result:
A study of data availability, reusability, and reproducibility demonstrated that well-described, ``in principle reusable'' data often does not suffice to reproduce the scientific findings of the corresponding publications due to  missing process provenance metadata \citep{hardwicke2018data}.
Secondly, even if methods and their sequence are well-described, precise information about the employed software tools is crucial, too.
The fact that different neuroimaging analysis software can produce distinct results from the same data despite using similar conceptual methodology is well known \citep{bowring2019exploring}.
This has been attributed to implementation differences \citep{palumbo2019evaluation}, software errors \citep{eklund2016cluster}, or analytic configurations \citep{pauli2016exploring}.
For example, in task-based fMRI, \citet{li2021moving} found that the choice of output space or resolution can have a marked impact on variability between conceptually similar processing pipelines.
Moreover, even with identical pipelines and data, surprising result variability can occur with minor variations in parametrization.
\citet{mueller2017commentary} reported that the choice of resampling resolution impacts alpha inflation, and \citet{li2021moving} identified the decision whether or not to include global signal regression as a major source of intra-pipeline variation.
Finally, even the same analysis, with identical parametrization, software tool, and data, can result in different outcomes if it is repeated across different operating systems, or with differences in versions of a singular software tool or operating system \citep{gronenschild2012effects, glatard2015reproducibility}.
%more here
Computational reproducibility across computing environments thus often remains elusive unless accounted for from the very start.
Therefore, in addition to ``Everything matters'',  \citet{kennedy2019everything} cued the phrase ``Reproducible by Design (as opposed to reproducibility as an afterthought)'' for conducting research in a way that makes computational reproducibility possible.
The next section highlights a number of strategies for this.

% However, a growing number of studies suggest that differences in the implementation of these processing steps or how they are “glued together” can yield notably different outcomes. Studies systematically comparing specific preprocessing steps such as segmentation15, motion correction16, and registration17–19 have reported substantial variation in outputs generated across independently developed packages when applied to the same data. In the analysis of task fMRI data, end-to-end pipelines built using different software packages have been found to produce marked variation in the final results20–23. from https://www.biorxiv.org/content/10.1101/2021.12.01.470790v2.full


\section{Towards re-usable research objects}


The reusability of research objects has become a distinct characteristic of scientific practice as it allows for reproduction, verification, building up upon and extending existing work, evidence synthesis, and minimizing duplicate efforts in the advancement of science \citep{thanos2017research}.
With this, it maximizes the impact of the funding and work that resulted in the research output.
Its central role in the \gls{FAIR} principles \citep{wilkinson2016fair} and a variety of funding sources such as the Economic and Social Research Council (ESRC, UK), the European Research Council (ERC, EU), or the National Institutes of Health (NIH, US) are a testament to this.
In the scope of the FAIR principles, reusability focuses on the ability of a human or a machine to decide if data are useful and usable in a particular context.
This reusability requires trust \citep{bechhofer2013linked}: Re-users must be able to audit the steps performed in an experiment or analysis in order to be convinced of the validity of the results or derivatives.
FAIR principle R1.2, ``(Meta)data are associated with detailed provenance'' \citep{wilkinson2016fair}, refers to this.
This principle also encodes the process provenance necessary for reproducibility.
And indeed, reproducibility and trust are closely related:
Where resources are not fully FAIR yet, manual reproducibility typically yields the trust that process provenance would otherwise provide.
A new project, for example, commonly starts with a check if the previous foundational findings still hold.\\
As the FAIR principles advocate for richly curated metadata, many scientific fields or projects argue in favor of coordinated use of ontologies for metadata and brought forward efforts for ontology development and consensus building \citep[e.g.,][]{wise2019implementation, abrams2022standards, papadiamantis2020metadata}.
But not in all domains are the necessary metadata standards incentivized or ready to use.
A few years before the publication of the FAIR principles, \citet{bechhofer2010research} cued the term ``reusable research object'' in a conceptual position paper.
They describe it as what can now be seen as a precursor of a FAIR research object, specifically as a ``container for a principled aggregation of resources, produced and consumed by common services and shareable \textit{within} and \textit{across} organisational boundaries [...that] includes not only the \textit{data} used, and \textit{methods} employed to produce and analyse that data, but also the \textit{people} involved in the investigation. An association with a dataset (or service, or result collection, or instrument) is now more than just a citation or reference to that dataset (or service or result collection). The association is rather a \textit{link} to that dataset (or service or result collection) that can be explicitly followed or dereferenced providing access to the actual resource and thus enactment of the service, query or retrieval of data, and so on.'' \citep{bechhofer2010research}.
This description matches characteristics that DataLad datasets or its contents can posses.
% Detail how datalad adheres to these requirements.
In the following, I will highlight four properties of research objects that can arise from pragmatic research data management -- versioned, actionable, modular, and portable -- and how these properties make them reusable  even if full FAIRness can not yet be achieved \citep{wagnerohbm2021}.
% and argue why the \gls{rdm} features that DataLad provides assist with FAIRification.

\subsubsection{Exhaustive versioning}

\citet{bechhofer2010research} stress that a reusable research object is an ``all-encompassing'' resource, including every relevant digital artifact of the project.
DataLad datasets lend themselves well to this.
As they can track files of any size or type, they are a suitable overlay structure to include every relevant element for a scientific project, laying the foundation to include all digital data, methods, and provenance.
But beyond this, their ability to version control adds an important additional feature to exhaustiveness.
The information ``I generated X from data Y with software Z'' is insufficient for reproducibility and trustworthiness if Y exists in multiple versions or subsets, if different releases of Z have relevant implementation differences, or if Z behaves differently depending on the environment it is used in.
If digital research objects are exhaustively tracked, they can be accessed and used transparently in a uniquely identified version state.
This identity registration removes ambiguity that arises if the files in question are not completely static.
Therefore, the first relevant feature that DataLad provides for reproducibility and reusability is exhaustive version control for all relevant files -- from data to code to software environments.
% make point about **exhaustive** tracking stronger

\subsubsection{Actionable metadata}

Outside of the quote cited above, \citet{bechhofer2010research} further denote the ability to repeat, reproduce, and validate a process as characteristics of reusable research objects:
``There should be sufficient information in a Research Object for the original researcher or others to be able to repeat the study, perhaps years later.
[...]
[Reproducibility] can be seen as a special case of Repeatability where there is a complete set of information such that a final or intermediate result can be verified.
In the process of repeating and especially in reproducing a study, we introduce the requirement for some form of comparability framework in order to ascertain whether we have indeed produced the same results.
[...] provenance, and being able to audit experiments and investigations is key to the scientific method.
Third parties must be able to audit the steps performed in an experiment in order to be convinced of the validity of results'' \citet{bechhofer2010research}.\\
In common scientific practice, though, process provenance metadata how a file came to be is still often incomplete and difficult to retrace \citep{hardwicke2018data}.
As it is tedious, often without an immediate benefit for curators, and rarely explicitly incentivized to retrospectively annotate research objects with process provenance \citep{edwards2011science, san2009long}, it should rather be captured at the time of creation, with the tools and persons that are involved in the creation of research outputs anyway \citep{dallas2016digital}.
But while early curation can increase the \textit{comprehensiveness} of metadata, additional measures should guarantee its \textit{validity}, as even fully described research outputs fail to be reproducible or reusable if their description or provenance contains errors \citep[see, e.g.,][]{manninen2017reproducibility}.
The most pragmatic approach to validate metadata is to base subsequent processing on them.
For a simple example, consider a codified parametrization of an analysis in a configuration or analysis design file  \citep[see, e.g.,][]{jas2018reproducible}:
If an analysis based on it completes successfully, it constitutes valid provenance metadata, created by an expert or automatically at the earliest possible time at no additional cost, and adds immediate benefit for curators as it captures relevant provenance and detects erroneous or missing metadata in passing.
Creation and validation is easiest if it is an automatic process within the research process, and if the tools used during the creation also use the same metadata that gets eventually published alongside the final research output.
Therefore, the actionable metadata DataLad can acquire from command executions are the second relevant property for reusability.
Even if this metadata does not follow established community standards as required by the \gls{FAIR} principles yet, it preserves knowledge that would otherwise be lost, without requiring additional training, impeding later additions, or putting additional burden on scientists -- it is a byproduct of standard scientific practice.
The fact that it can be re-executed automatically, and that resulting recomputations are automatically compared to previous versions by employed version control tools, provides an effortless form of validation.


\subsubsection{Modular structure}

The reusability of scientific work can improve if it is accessible in modular units that constitute unambiguous multi-use components, such as raw data, processed data, or software.
In the simplest case, modularization means placing conceptually distinct content into separate files, and grouping files in individual directories to reflect more global structures.
Distinct units, such as the directories ``code/'' and ``inputs/'', increase transparency if each location is associated with distinguishable content, ease flexible recombination of such components into new projects, allow continuous evolution of an individual module without impact on other components of a project, and enable location-specific access control.
Though a single modular unit can not entail all relevant elements of a scientific study or data analysis, exhaustive tracking of all elements without sacrificing modularity can be achieved by linking multiple modular units in dependency relationships.
As \citet{bechhofer2010research} write above, associations to other research objects should be more than ``just a citation or reference'', but an actionable link that provides access to whichever resource it refers to.
A useful metaphor that match this description are package management systems such as conda (\url{https://docs.conda.io}) or APT (\url{https://wiki.debian.org/Apt}): A software package is a modular unit, installed with a package manager.
However, packages usually depend on other software packages, which are listed as its ``dependencies''.
During installation, package managers check if all of the linked dependencies exist on the system, and if not, install them in the required versions automatically.
For scientific projects, we can conceptualize its modular units (data, software, code) as the dependencies of a given research output.
When those units are tracked as datasets, dependency relationships in the form of subdatasets form actionable links with precise versions.
Thus, a third feature for reusability is modularity as provided with DataLad's subdataset mechanism.
In a superdataset, a registered subdataset is known with a location and version identifier.
On demand, it can be installed precisely as needed, from whichever location or service it is available from, similar to the process that \citet{bechhofer2010research} envisioned.


\subsubsection{Portability}

\citet{bechhofer2010research} further highlight the ability to share reusable research objects within and across organizational bounderies.
The fourth and final property DataLad datasets entail for reusability and that fits to this description is portability.
The more portable a digital research object is, the easier it is to reuse it.
A research object is fully portable if no adjustments are necessary for it to function the way it is intended to on different computational infrastructure -- ideally even when used by a naive re-user with a different area of expertise (i.e., without domain knowledge).
The more adjustment or domain knowledge is necessary, the less portable a research output becomes.
As a simple example for what this would entail, consider custom scripts for an analysis.
If they internally reference the file system outside of their DataLad dataset, or refer to locations with specific absolute paths,  they could not run on a different computer.
A re-user would need to adjust the scripts, or their system, to match the expectations implicitly encoded into the script, which impedes reusability considerably.
Only if a re-user does not need to modify project files, they can be certain that they did not inadvertently break or influence the output with it.\\
Achieving the previous properties in a research output is work towards portability.
A factor contributing to portability is self-containment such that research outputs can be used or reproduced on different computational infrastructure by other users without requiring additional elements outside of it.
Exhaustive versioning and modularization pave the way to include all necessary code, data, and computational environments into a complete, self-contained structure that accompanies a research output.
If a user ensures that a project can be moved across computers and remains functional without adjustment, for example by ensuring that no references to file system, operating system, or user specific idiosyncrasies are included, portability is improved further.
And with actionable process provenance that can be re-executed, even verification and re-use even without domain knowledge becomes possible.


\begin{figure}
	\centering
	\includegraphics[width=.9\textwidth]{vamp.png}
	\caption[DataLad datasets as reusable research objects]{Reusing research outputs implies trust, and trust should be earned through verification. DataLad datasets possess features that assist with this aim. Verification is enabled by provenance information. Provenance capture of research outcomes requires \textbf{exhaustive versioning} of all inputs, as well as process records that describe how inputs were combined and transformed to generate outputs (middle). When lightweight metadata are \textbf{actionable}, they can be used to reproduce research outputs in a different environment from precisely identified inputs by (re-)applying the recorded process (left). A data structure that affords this type of \textbf{portable} recomputation is a self-contained unit that can be reused as a \textbf{modular} input component for incremental research (right).
	}
	\label{fig:vamp}
\end{figure}




% curation needs to be pragmatic

Although FAIR research objects are universally desirable, in practice, the necessary standards and procedures for creating FAIR (meta)data are not always already in place when research is conducted.
This can turn FAIRification into a bureaucratic data governance effort, diminishing the immediately obvious benefits for the curator \citep{zehl2016handling}.
However, the four properties outlined above are a big step into the right direction, and essentially a byproduct from pragmatic research data management in DataLad datasets.
\cref{fig:vamp} illustrates how it yields trusted and reusable research objects, and the next section details a technical implementation and proof-of-concept analysis to create such reusable research objects as a byproduct of \gls{rdm} in analyses of any scale.

\pagebreak

\section{FAIRly big: A framework for computationally reproducible processing of large-scale data}

The following section is a short overview of our original publication \citet{wagner2022fairly}. The reader is invited to visit the published paper for further details.

Large-scale datasets pose additional challenges for \gls{FAIR} research objects.
Their storage and computational demands increasingly exceed common \gls{HPC} infrastructure, and computing procedures that are common in fields accustomed to smaller datasets such as storing multiple copies of the data become infeasible \citep{horien2021hitchhiker}.
As the complexity of reproducing and verifying large scale datasets growths, the trustworthiness of derivative data decreases.
And as data processing results often multiply storage demands, keeping intermediate outcomes on disk is rendered increasingly prohibitive, which further impedes the possibility to retrace the origin of research outcomes.
Yet for large scale datasets specifically, sharing data derivatives is the most -- or sometimes the only -- viable way to extend previous research \citep{craddock2013neuro}:
It opens up research opportunities to scholars without access to adequate computational resources, and minimizes duplicate analysis efforts for resource-heavy, costly computations with considerable environmental impact \citep{portegies2020ecological}.\\
Based on DataLad, containerization software, and job scheduling systems, we build a portable, free and open source framework for scalable reproducible processing.
It applies workflows from software engineering -- in particular distributed development -- to computational research, and empowers reusers to automatically reproduce results based on machine-actionable records of computational provenance without access to the original infrastructure.
For this, it puts the aforementioned properties in practice, using DataLad datasets as a comprehensive data structure to track all elements of digital processing, perform portable computing in automatically bootstrapped ephemeral workspaces, and capture validated and re-executable process provenance records.

\subsection{Framework overview}


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{ukbworkflow_simplified.pdf}
	\caption[FAIRly big: Framework overview]{Framework overview. a) A DataLad dataset links input data and pipeline, and contains computed results with actionable process provenance and location information post-computation, allowing on-demand file retrieval or recomputation. b) Process-flowchart: First, a DataLad dataset exhaustively tracks required processing components as modules (e.g., input data, processing pipeline). Next, compute jobs are executed, if possible in parallel, capturing and validating provenance required for computation. Finally, results and provenance from parallel executions are merged. c) An ephemeral (short-lived) compute workspace for validating process provenance. The temporary, lean clone retrieves relevant subsets of data based on its parametrization and information recorded in the dataset. Upon success, process provenance and results are pushed into permanent storage (see d), and the ephemeral workspace is purged. d) The internal dataset representation in a RIA store: The store receives results and can contain input data, optionally using compressed archives or encryption during storage and transport. It is the only place where results take up permanent disk space. License: CC-BY-SA, \citet{wagner2022fairly}}
	\label{fig:fairly_workflow}
\end{figure}


The framework combines distributed version control, containerization, job scheduling, and storage solutions with optional encryption and compression into a sequential analysis workflow (\cref{fig:fairly_workflow}):\\
The start and end point of the workflow is a portable DataLad dataset that contains the exact identity and location of all data processing inputs and, eventually, re-executable processing results (\cref{fig:fairly_workflow}a).
In a first step, it is assembled from all relevant processing components, most commonly input data and software containers with computational environments or pipelines (\cref{fig:fairly_workflow}b).
The use of software containers, while not strictly required, is a practical method to provide portable computational environments and forgo a number of challenges that computational reproducibility otherwise poses.
To harvest the advantages of modularity, processing components are placed in separate DataLad datasets and linked as subdatasets.
To provide features that are necessary for processing personal or large-scale data, such as encryption (to comply with data protection regulations), compression (to save disk space), and composition into archive files (to reduce the number of files),
DataLad datasets involved in the workflow are placed into so-called RIA stores \citep{poldrackRIA}.
This ``backend'' representation facilitates programmatic data management and reduces storage demands by storing a dataset of any size in 25 files, with optional compression and encryption.
\cref{fig:fairly_workflow}a illustrates a DataLad dataset's front-end structure when cloned in a user's workspace, and \cref{fig:fairly_workflow}d shows its RIA representation.\\
Based on this self-contained structure, the framework needs to conduct user-defined processing in a way that captures and validates actionable metadata.
While DataLad's \texttt{containers-run} functionality provides provenance capture, simultaneous validation requires additional tweaks.
To achieve it, the framework bootstraps an entire temporary analysis environment from scratch based on the information contained in the initial dataset (\cref{fig:fairly_workflow}c).
In this ephemeral environment, processing can only be based on information already recorded in the dataset or provided in the execution command, which is invoked with a provenance capturing \texttt{datalad run} or \texttt{datalad containers-run}.
It specifies required inputs and expected outputs by their relative path in the DataLad dataset hierarchy.
If the computations succeeds, two validity guarantees can be derived regarding the provenance of the computational outcomes: 1) All dataset modifications can be causally attributed to the initiated computation; 2) only declared inputs were required to produce the outcomes.
This constitutes evidence that existing information is valid and sufficient to make the dataset portable.
\cref{fig:fairly_metadata} illustrates this provenance, how it is captured, and how it can be used for recomputation.\\


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{coarsemetadata.pdf}
	\caption[FAIRly big: Process provenance of an individual job]{Process provenance of an individual job, its generation, and re-execution. a) A \texttt{datalad containers-run} command with a container name specification (cat), a container parametrization or command, a commit message, and an input and output data specification generates actionable process provenance as a structured record linked to a Git commit. b) To re-execute it, \texttt{datalad rerun} needs to be parameterized with a revision identifier, such as a Git tag, a ``commit shasum'' (\texttt{e035f896s45c9f}), or a revision range containing one or more commits with associated provenance records. c) As the core execution command (see Listing 1, line 24-30) at the center of each individual job, it performs data retrieval, container execution, and result capture, and generates the actionable provenance. d) The provenance record stored alongside computed results in the revision history. A legend (right) highlights the most important pieces of recorded provenance. It is re-executable with DataLad only, but sufficient information to repeat a computation using other means can also be inferred from the structured JSON records by other software or even humans. License: CC-BY-SA, \citet{wagner2022fairly}}
	\label{fig:fairly_metadata}
\end{figure}


The basis for ephemeral workspaces is the ability to distribute DataLad datasets across local or remote infrastructure as lightweight, linked clones, and the process mirrors a software development routine, where changes are developed in a distributed network:
An orchestration layer clones the DataLad dataset into a temporary location, uses its recorded history to infer relevant details about precessing components, and add results on top of it.
Afterwards, results and their provenance can be pushed back before the ephemeral workspace is purged.
To ensure that this orchestration can scale, it needs to support parallel executions, i.e., several analyses running in ephemeral workspaces at the same time.
This is complicated in Git repositories as concurrent processes can interfere with one another and simultaneous commits can cause conflicts.
The use of file locking, a mechanism to control concurrent access to files by enforcing the serialization of updates, can limit the number of processes that modify specific locations on the file system and prevent read operations while a modification takes place \citep{xoxa2015implementations}.
Beyond that, two procedural techniques, again adopted from distributed software development, make the orchestration more efficient.
The first consists of duplicating the DataLad dataset that is used as an analysis starting point into two clones: One remains unaltered and acts as an input source from which the ephemeral clones are bootstrapped.
The other one acts as the target location for results (\cref{fig:fairly_workflow}d).
This separation prevents concurrent clone and push operations.
Moreover, as the clone location does not receive any results, the dataset is always at the leanest state, which ensures fast cloning.
The second technique involves the use of branches, independent segments of a DataLad dataset's history that allow for parallel developments based on a common starting point.
Each individual ephemeral workspace saves its results and provenance on a unique branch.
When several ephemeral clones push their individual histories into a single location, they thus do not conflict with one another.
This process mirrors feature development in software projects where a mainline branch contains agreed upon code.
Changes are build on top of the mainline code, but in branches such that concurrent developments of several contributors do not conflict, and later, a so-called \texttt{merge} can integrate a branch into another one.
The clone and push orchestration is implemented as a job script that wraps the desired processing.
\cref{lst:job} contains an example bash script:
To set up the ephemeral workspace and push target, it receives the source dataset for cloning and result dataset for pushing as parameters (lines 5-6), clones the source into a temporary location (line 9), registers the push target (line 12), and creates a unique branch (line 14).
Afterwards, analysis-specific code can run with provenance tracking, and as a last step, its results and provenance are pushed into the destination dataset (lines 34, 38, \cref{fig:fairly_workflow}1b, blue arrow).\\

\begin{Listing}
	\centering
	\lstset{
		language=bash,
		basicstyle=\ttfamily\footnotesize,
		breakatwhitespace=true,
		breaklines=true,
		breakindent=-0pt,
		postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow\space}},
		captionpos=b,
		commentstyle=\color{dataladblue!60!black},
		firstnumber=1,
		keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
		%keywordstyle=\color{blue!70!black},
		numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
		numbersep=3pt,                   % how far the line-numbers are from the code
		numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
		rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
		showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
		showstringspaces=false,          % underline spaces within strings only
		showtabs=false,                  % show tabs within strings adding particular underscores
		stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
		stringstyle=\color{dataladyellow!70!black},     % string literal style
		tabsize=2,
		xleftmargin=1em,
	}
	% our separation of input and output store still bothers me, this could have
	% been avoided with a bit more thinking, but oh well, too late now
	\begin{lstlisting}[multicols=2]
		#!/bin/bash
		# fail on any issue, show commands
		set -e -u -x
		# name arguments for readability
		dssource="$1"
		pushgitremote="$2"
		subid="$3"
		# obtain the source dataset
		datalad clone "${dssource}" ds
		cd ds
		# register location for result deposition
		git remote add outputstore "$pushgitremote"
		# create job-specific branch for results
		git checkout -b "job-$JOBID"
		# START OF APPLICATION-SPECIFIC CODE
		# pull down input data manually,
		# only needed for wildcard-based file
		# selection in the next command
		datalad get -n "inputs/ukb/${subid}"
		# datalad containers-run executes
		# the "CAT" pipeline. Specified
		# inputs are auto-obtained, specified
		# outputs are saved with provenance
		datalad containers-run \
		-m "Compute subject ${subid}" \
		-n cat \
		--explicit \
		--output "${subid}" \
		--input "inputs/ukb/${subid}/*T1w.nii.gz"
		"<container invokation arguments>"
		# END OF APPLICATION-SPECIFIC CODE
		# push result file content to the
		# configured "storage-remote"
		datalad push --to storage-remote
		# push branch with provenance records
		# needs a global lock to prevent
		# write conflicts
		flock "$DSLOCKFILE" git push outputstore
		# log entry to mark non-error exit
		echo SUCCESS
	\end{lstlisting}

	% max 250, is 190
	\caption[Job orchestration for parallel processing]{Processing orchestration in a bash script.
		A batch system invokes it in a temporary working directory with three parameters:
		a URL of a source DataLad dataset,
		a URL to deposit job-results at, and
		an identifier to select a sample for processing.
		%
		The job implementation conducts three main steps:
		1)~\texttt{clone} a DataLad dataset with all information to bootstrap an ephemeral computing environment for each job;
		2)~\texttt{containers-run} a containerized pipeline with a comprehensive specification of to-be-retrieved inputs and to-be-captured outputs;
		3)~\texttt{push} captured outputs (concurrency-safe; line 34) and process provenance records (not concurrency-safe, protected by a file lock; line 38) to a permanent storage location.
		%
		Jobs are independent and can run in parallel. Potential job configurations can be supplied using batch system specific means (e.g., \texttt{DSLOCKFILE} and \texttt{JOBID} environment variables).
		Processing can be adjusted with the container invocation.}
	\label{lst:job}
\end{Listing}


One script execution equals one processing in one ephemeral workspace.
By splitting the computation into parts, operations can be parallelized.
A common example is one processing pipeline per participant, i.e., the parallelized execution of a processing pipeline on different, independent parts of input data.
\cref{lst:job} exemplifies this: A specific participant identifier, provided as a parameter (line 7), determines the subset of input data a processing pipeline runs on (\cref{fig:fairly_workflow}c).
Precisely how to split a specific computation is dependent on its nature, but as parallelization often corresponds to the granularity at which a recomputation will be possible in our framework, relevant considerations are, for example: ``What is the smallest unit for which a recomputation is desirable?'', or ``For which unit size is a recomputation still feasible on commodity hardware?'' \citep{wagner2022fairly}.
Once a fitting granularity is determined, a job scheduling system can assist in deploying thousands of computations in parallel.
To automate this process, the framework supports to major job schedulers, HTCondor \citep{thain2005distributed} and SLURM \citep{yoo2003slurm}, natively.
Their configuration is highly infrastructure-specific, and must determine an optimum balance of resource demands, such as run time, memory and storage requirements, in order to achieve optimal throughput.
Importantly, the final process provenance neither contains the orchestration of ephemeral workspaces, nor the scheduling layer.
For a original computation, these layers can thus contain necessary platform-specific tuning without impacting the portability of the final research object.
This means DataLad, a software container tool, and a basic UNIX shell environment are the only requirements for recomputing captured outputs, and recomputation on systems with different batch scheduling software is possible by providing alternative job specifications, without changes to the pipeline implementation.\\
If the necessary processing elements exist in suitable formats (e.g., DataLad datasets with processing components, scripts stripped of platform idiosyncrasies, and so forth), the above setup can be created automatically with an openly shared bootstrapping script\footnote{\url{www.github.com/psychoinformatics-de/fairly-big-processing-workflow}.}.
After setup, two things are left to do for a user:
Start the processing, and, once the results from all ephemeral workspaces are aggregated in branches in the target dataset, consolidate them the mainline branch (\cref{fig:fairly_workflow}b, ``Result consolidation'').
In the simplest case, all compute jobs produced non-intersecting outputs, i.e., no single file was written to by more than one compute job.
In this case, all branches can be merged at once using a so-called octopus-merge.\\
The outcome of this consolidation process is a self-contained DataLad dataset, with valid, machine-actionable provenance information for every single result file of the performed data processing.
As such, it is a modular unit of data, suitable as input for further processing and analysis.
And it is a practical example how the four properties -- exhaustively versioned, actionable, modular, and portable -- translate to trustworthy, reusable research outputs.
Overall, our framework is a general-purpose solution that is compatible with any data that can be represented as files of any size, and any computation that can be performed via a command line call. It is built on a collection of portable, free and open-source tools that can be deployed without special privileges or administrative coordination on standard HTC/HPC infrastructure, or personal computing equipment.
An open tutorial and result showcase is publicly available at \url{www.github.com/psychoinformatics-de/fairly-big-processing-workflow-tutorial}.
To test its capabilities further, we conducted a proof-of-concept analysis on one of the largest neuroscientific datasets to date.

%\lstset{
	%	language=bash,
	%	basicstyle=\ttfamily\footnotesize,
	%	commentstyle=\color{dataladblue!60!black},
	%	showstringspaces=false,          % underline spaces within strings only
	%	stringstyle=\color{dataladyellow!70!black},     % string literal style
	%}
%\begin{lstlisting}
%	# octopus-merge all "job" branches at once
%	git merge -m "Merge results" $(git branch -al | grep 'job-')
%\end{lstlisting}

%To ensure reproducibility for an audience that does not have access to the original infrastructure, what-to-compute needs to be infrastructure-agnostic, without references to system-specifics such as absolute paths, or programs and services not tracked and provided by the DataLad dataset itself. Then, computation and recomputation of what-to-compute are possible on different systems, with any potential adjustments only relating to the job orchestration layer in how-to-compute.


\subsection{Proof-of-concept analysis}

In a proof-of-concept analysis, we applied the framework to run a containerized pipeline for \gls{vbm} \citep{ashburner2000voxel} from the \gls{CAT} \citep{gaser} on data from the UKB project \citep[][comprising 76 TB in 43 million files under strict usage constraints]{matthews2015uk}.
In doing so, we assessed the framework's scalability and if it can be used across different infrastructures, investigated result variability between two recomputations of the pipeline, and, in order to demonstrate that the framework can capture and re-execute complete process provenance, we also recomputed individual results on a personal laptop.
An overview of the analysis is shown in \cref{fig:fairly_datasets}, and the setup steps are publicly available as a bootstrap script\footnote{\url{https://github.com/psychoinformatics-de/fairly-big-processing-workflow/blob/main/bootstrap_ukb_cat.sh}}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{ukb_datasets.pdf}
	\caption[FAIRly big: Overview of the proof-of-concept analysis]{ Overview of DataLad dataset linkage through processing and reuseAny DataLad dataset may comprise other DataLad datasets as subdatasets via lightweight but actionable and versioned links. This connects a dataset to the content and provenance of a different modular unit of data, such as the outcomes of a the preceding processing step. The genesis of an analysis output (Analysis A/B) based on intermediate processing outcomes (Tailored results A/B) can thus be traced back all the way to the original raw data. Access control and storage choices are independent across individual components in this network of linked data modules. Aggregated data and analysis results can be shared with larger audiences or publicly on a variety of platforms, while raw and derived data may underlie particular access restrictions, or require particular hosting solutions due to their size. License: CC-BY-SA, \citet{wagner2022fairly}}
	\label{fig:fairly_datasets}
\end{figure}

\subsubsection{Workflow setup}

First, using the DataLad extension \texttt{datalad-ukbiobank} \citep{hanke_michael_2022_7296550}, we retrieved \gls{MRI} data into one DataLad dataset per participant and restructured it to BIDS \citep{gorgolewski2016brain}, yielding 42,715 datasets in total.
A single superdataset (``Data'' in \cref{fig:fairly_datasets}) tracks them using about 40 MB of space.
It is installable within seconds to retrieve any registered file content in the DataLad dataset hierarchy on demand.\\
To set up a processing pipeline, we built a Singularity container for the Computational Anatomy Toolbox \citep[CAT; version: CAT12.7-RC2, r1720]{gaser}, which is an extension to the Statistical Parametric Mapping software (SPM; version: SPM12, r7771; \url{www.fil.ion.ucl.ac.uk/spm/software})\footnote{A detailed description and full recipe of the container together with instructions on how to build and use it is publicly available at \url{github.com/m-wierzba/cat-container}.}.
From it, we chose \gls{CAT}'s default segmentation of structural T1-weighted images using geodesic shooting \citep{ashburner2011diffeomorphic}, including calculation of total \gls{GM}, \gls{WM}, and \gls{TIV}, as well as extraction of regional \gls{GM} estimates from several brain parcellations.
We then linked these two inputs as subdatasets to the processing dataset (``Results'' in \cref{fig:fairly_datasets}), and added two custom code files.
First, a batch script with \gls{CAT}'s processing steps and parameterization, which bundled up all relevant analysis steps into single command.
This script runs per image, and constitutes the smallest unit for recomputation.
And second, a utility script to post-process all relevant outputs with a focus to reduce the number of files and artificial variations between recomputations.
For this, it stripped results of timestamps and other non-deterministic log file content, and used the \texttt{tar} utility to bundle \gls{vbm} results into four reproducibly organized archives according to envisioned consumption scenarios (see the files in \cref{fig:fairly_workflow}a).
These measures were taken to obtain a meaningful estimate of intra-pipeline variability across recomputations, and to keep the number of files in the resulting dataset in a manageable range.\\
%After all required processing elements were either included or linked, we stored all DataLad datasets in a RIA store:
%A single participant dataset with several hundreds of files is represented in 25 inodes and about 4 GB of disk space in a RIA store, and in total, the employed RIA store hosts 42,715 datasets comprising the full UKB data with less than 940k inodes.\\
To test the frameworks portability and estimate result variability across computations, we set the analysis up to run on two systems, a \gls{HPC} cluster and a \gls{HTC} cluster.
The two systems used SLURM or HTCondor for job scheduling, respectively, and each system imposed different resource constraints.
The \gls{HPC} system, a modular supercomputer with abundant disk space and compute power \citep{krause2018jureca}, imposed an inode quota, a limit on the total number of files, of 4.4 million – less than the total number of files of the raw dataset.
The \gls{HTC} cluster, in contrast, was constrained on storage capacity, preventing the existence of more than one copy of the raw dataset, and limiting the size of derivatives that could be stored.
The framework was thus setup identically across the employed systems apart from the implementation of job submission, which accounted for platform specific idiosyncrasies such as these restrictions.
Irrespective of system, one compute job per participant was generated.
This compute job serially processed all available anatomical images (0, 1, or 2) for a given participant.
For each image, a dedicated provenance record was captured, yielding a total of 41,180 records.
The platform-specific job submission scripts adjusted the job load to the available disk space or inode resources when disk space or inode availability were insufficient for the full dataset by scheduling only as many jobs in parallel as there were resources to handle extracted inputs.

\subsubsection{Workflow execution and consolidation}

On the \gls{HPC} system we completed data processing for the one-hour-per-image pipeline within 10.5 hours, using 25 dedicated compute nodes, each executing 125 jobs in parallel on RAM disks with GNU Parallel \citep{tange2011gnu}.
On the \gls{HTC} system in turn, computations were scheduled dynamically across several weeks using HTCondor.
On both systems, recorded outputs and provenance records amounted to a total of 995.6 GB of computed derivatives in 163,212 files.
After completion, results were consolidated by merging result branches, yielding a collection of different VBM-related measures for all images in the sample, represented in archives, and annotated with re-executable provenance records in a DataLad dataset.\\
This DataLad dataset was then subsampled into smaller ``special purpose'' datasets tuned for specific research questions, containing extracted and optionally aggregated subsets of the results for easier consumption and faster access.
For this, the main result DataLad dataset became an input to a new tailored DataLad dataset via nesting (``Tailored results A/B'' in \cref{fig:fairly_datasets}), which extracted and transformed required files with provenance tracking using \texttt{datalad run}.
If underlying large-scale computation is redone or extended, the subsampled datasets can be updated by re-applying this transformation with a \texttt{datalad rerun}.
And throughout the dataset hierarchy -- using the encoded, machine-actionable provenance information -- a single result can be traced to the precise files they were generated from in a transparent and reproducible manner.
The direct computational output of the proof-of-concept framework execution is therefore not a final result, but an intermediate representation optimized for storage and handling.
As more tailored views for concrete use cases can be flexibly and reproducibly created, we achieve a compromise between the desires of a data consumer and the demands of the storage infrastructure and operators for such large scale datasets.

\subsubsection{Intra-pipeline variability}

Pipeline replication can play in important role for reproducible research as a means of exploring analytic variation and assessing the robustness of findings \citep{li2021moving}, and our framework is particularly convenient for this application.
In order to investigate intra-pipeline variability, the result consolidation described above was first performed separately on each computational infrastructure.
Afterwards, the two complete sets of results were integrated in the same dataset, as two different branches, to estimate result variability.
With the exception of execution time, the number of jobs, proportion of successful jobs, and size and structure of the results were identical between the two systems\footnote{The difference in computational performance can be seen in a visualization of provenance information at \url{youtube.com/watch?v=UsW6xN2f2jc}. This visualization was awarded second place in the category videos \& animations in the 2021 \gls{ohbm} Brain Art Competition.}.
As content identity is precisely captured, bit-identical recomputations are easy to identify.
One type of the created output archives, surface and thickness projections, never yielded bit-identical results across computations.
But among the other three types of archives (modulated gray matter density and partial volume estimates in template space, atlas projections and partial volumes in individual space, and regional volume and thickness estimates of several atlases/parcellations), more than 50\% of all output files were bit-identical across the two computations.
A closer investigation of non-identical results revealed that outcome variability was largely attributable to minor numerical differences.
We illustrated the amount of dissimilarity by computing the \gls{mse} over recomputations for a range of key VBM estimates.
They can be found in \cref{tab:fairly_mse}.
We also correlated \gls{vbm} estimate distributions across recomputations for different brain parcellations included in the CAT toolbox output.
The lowest observed correlation were Pearson’s $\rho > 0.998$ for the Destrieux 2009 surface parcellation \citep{destrieux2010automatic} for all brain regions.
Finally, we derived quality control metrics for anatomical images from the computed results \citep{dahnke_retrospective_2013,dahnke_quality_2015} and correlated them across results.
They exhibit $\rho > 0.99999$ for computation and recomputation.


\begin{table}
	\centering
	\begin{tabular}{lcc}
		\toprule
		measure & $\mu$ & \gls{mse} \\ \midrule
		total surface area & 1891 & 0.315 \\
		cerebro-spinal fluid & 365 & 0.052 \\
        total intracranial volume & 1508 & 0.052\\
        white matter & 519 & 0.004\\
        gray matter & 621 & 0.001\\
		\bottomrule
	\end{tabular}
	\caption[Mean and mean squared error of results across computations]{Mean and mean squared error of results across computations}
	\label{tab:fairly_mse}
\end{table}


\subsubsection{Infrastructure-agnostic reproducibility}

The potential for full computational reproducibility not only permits structured investigations of result variability, but also increases the trustworthiness of the research process per se.
To confirm computational reproducibility for a consumer, we performed an automatic recomputation of individual results on a personal laptop with much lower storage and compute capacity than a cluster, and without job scheduling software.
This type of spot-checking results resembles the scenario of an interested reader or reviewer of a scientific publication with access to (parts of) the data, but no access to adequate large-scale computing resources.
The recomputation solely relied on the local availability of the Singularity container technology, but was otherwise fully automatic.
Its success confirmed that platform idiosyncrasies were confined to the outer job scheduling layer only, and that the resulting research object was portable and across infrastructures.


\subsection{Future steps}

(ADD SOME DISCUSSION?)]
Building up on the framework, a number of large-scale processing efforts as well as processing software tools have been created (CITE-BABS).



%From Donoho Buckheit 2015: Performance has everything to do with specifics: exactly what was done (which wavelets, which coders, which detectors, which corpus of data) with exactly what parameters. In this setting, publishing figures or results without the complete software environment could be compared to a mathematician publishing an announcement of a mathematical theorem without giving the proof. Waveleticians ought to publish their complete computational environments

\pagebreak



