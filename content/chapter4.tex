% !TeX root = ../main-english.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% -*- coding:utf-8 mod:LaTeX -*-

%This smart spell only works if no changes have been made to the chapter
%using the options proposed in preambel/chapterheads.tex.
\setchapterpreamble[u]{%
	\dictum[Albert Einstein]{If we knew what it was we were doing, it would not be called research, would it?}
}

\chapter{Reproducible brain state analyses}
\label{chap:k4}

The previous chapter focused on the technical challenges of reproducibility management, especially when applications need to scale to large samples, but it also highlighted readily available, pragmatic strategies to ensure portability and reusability.
A different challenge arises when the to be undertaken analyses are not entirely mapped out from the start, and a research project is exploratory in nature.
This upcoming chapter describes the central data analysis project in this thesis, studying working memory in sequential decision making.
While the majority of exploratory analyses I conducted in this project yielded null-results and were thus abandoned at an intermediate state, their setup used best practices and tools from \ref{chap:k1-rdm-2}, and followed the strategies and the framework outlined in chapter \ref{chap:k3}.
Corresponding project directories leave a transparent digital provenance trace, and analyses are portable across infrastructures.
Even if an analysis project does not yield a published article as its final outcome, intermediate research outcomes are a valuable stepping stone for further research or researchers.

\section{Project outline}

An exploratory neuroscientific project faces several challenges.
As outlined in Chapter \ref{chap:k1}, analyses are often multistepped.
But repeating a large number of processing step in all of many different analyses is computationally inefficient.
Furthermore, a large number of analyses makes it difficult to retain an intuitive structure.
When several different analyses are conducted in the same directory, researchers unfamiliar with the project struggle to associate inputs, code, software, and results.
But when different analyses are broken into distinct projects, the disk space demands of the underlying neural data usually multiply.
The distributed nature of the project across several institutions also demanded uncompromising portability.
Common exploratory approaches such as interactive computing or notebooks typically do not fulfill this requirement due to their short-lived and environment-dependent nature.
To overcome these challenges, research data management principles from the previous chapter were applied.
All analyses were setup as portable DataLad datasets.
The project evolution was captured in a linked hierarchy of datasets, starting with a BIDS-standardized raw dataset, continuing with a preprocessed derivative dataset, and spreading into multiple analyses applications based on a common starting point.

\begin{figure}
	\centering
	\includegraphics[width=.9\textwidth]{memento/analysis_setup.png}
	\caption[Memento analysis outline]{Analysis outline as linked DataLad datasets for provenance capture, disk usage reduction, and computational efficiency. A \gls{BIDS}-structured raw dataset is the anchor of all analysis. Based on }
	\label{fig:outline_memento}
\end{figure}
%
%\section{Theoretical background}
%
%
%In natural settings, value-based decisions under risk often have to be made without visual presence of competing alternatives.
%Do I commit to buying the red bicycle in the store I'm currently in, or should I return to the previous bike shop and pick the blue one?
%Do I want to rent the flat I viewed last Sunday, or the apartment I saw on Tuesday?
%In such situations, relevant properties of alternative options such as their subjective value have to be encoded, held in working memory through a maintenance (or ``delay'') period, and retrieved in order to form a categorical choice.
%To understand the neural processes underlying behavior in these situations, the neural activity in the delay period and how it might represent or maintain relevant properties were a focus of research in the last decades, but although several popular theories emerged over time and were either refuted or refined, the field yet lacks a comprehensive understanding \citep{sreenivasan2019and}.
%In the following, I will outline a brief overview of some major theoretical and methodological approaches.\\
%Early neuroimaging and electrophysiology studies suggested persistent neural activity (``spiking'') in specific brain areas during delay periods as a mechanism of working memory maintenance \citep{goldman1995cellular}.
%Sustained, above-baseline activity that starts during a sample presentation, lasts through the memory delay, and returns to base line activity after a response were found in prefrontal regions in human \citep[e.g.,][]{courtney1997transient} and non-human primates \citep[e.g.,][]{fuster1971neuron, funahashi1989mnemonic, miller1996neural}.
%And single-cell recordings from the prefrontal cortex in monkeys unveiled that individual neurons display activity that is selective to specific task-relevant aspects, such as task rules, spatial location, or stimulus features \citep{white1999rule, wallis2001single}, suggesting the existence of neurons that uphold working memory content with a fixed selectivity for certain properties.
%Others have pointed out, however, that persistent spiking in prefrontal areas could reflect a variety of other different processes, including decision making \citep{curtis2010beyond} or anticipation of the probe \citep{nobre2011attention}, that the high metabolic costs of action potentials is too energetically expensive to hold information in a spiking form \citep{attwell2001energy}, and that distractor tasks are able to remove the persistent activity without an impact on later retrieval, questioning its role in working memory maintenance \citep{larocque2013decoding, lewis2015neural}.\\
%The idea of neural selectivity to specific task-aspects was refined in the adaptive coding framework \citep{duncan2001adaptive}:
%Instead of a persistent, fixed selectivity, it postulates that neural responses are temporarily tuned to the particular task. This theory was able to explain how a recording of spiking activity in the same group of neurons relates to object information in one task but a location distinction in the next task \citep{duncan2001adaptive}.
%\citet{mongillo2008synaptic} proposed a highly influential theory according to which short-term neural plasticity changes are the underlying mechanism of working memory maintenance.
%Working memory maintenance is established via increased residual calcium levels at the presynaptic terminals of neurons, which causes a short-term synaptic facilitation, akin to synaptic weights that link neurons coding for a working memory item.
%With this facilitation, the memory can be transiently held for about 1s without enhanced spiking activity in a network of neurons.
%Building up on this theory,  \citet{stokes2015activity} proposed the dynamic coding framework, in which working memory is mediated by rapid transitions in such ``activity silent'' neural states.
%While these states mediate flexible, context-dependent processing they do not emerge as constant activity and rather as ``hidden states'', appearing as altered response sensitivities of neural networks, established via short-term and long-term synaptic plasticity and temporal functional connectivity changes that influence the response to stimuli.
%An explanation why spiking activity is nevertheless found during delay periods comes from findings that a task-irrelevant read-out or ``ping'' signal is able to reactivate the neural assembly \citep{trubutschek2017theory}.
%This raises the possibility that previous findings of neuronal spiking might have been similar readouts from an activity silent population after task-irrelevant non-specific inputs \citep{wolff2017dynamic}.
%Alternatively, \citet{fiebig2017spiking} proposed that occasional spiking of memory-encoding neurons is needed to refresh the activity-silent states.\\
%The idea that distributed representations that selectively favor information relevant to the current task emerge on the level of neural populations was central in other theories as well.
%\citet{rigotti2013importance} proposed the concept of mixed selectivity, in which information is distributed across neurons through non-linear and high-dimensional representations even when it is not observable in individual cells.
%The rise of multivariate methods such as decoding analyses gave way to thorough investigations of distributed neural patterns.
%\citet{king2014characterizing} formalized this with the temporal generalization method.
%In this method, several classifiers are trained on different time slices of training data each, and tested on all available times in the test data, thus revealing whether the neural code is stable or dynamically evolving.
%Interestingly, studies decoding working memory content found that decodable patterns can be non-stationary (Figure \ref{fig:dynamiccoding}).
%\citet{meyers2008dynamic} decoded object categories from electrophysiological data, and yielded worse decoding the more temporally distant training and testing times slices were apart.
%\citet{stokes2013dynamic} used temporal cross-correlation analyses to show that population responses in the prefrontal cortex evolve during a memory delay.
%These representations of task-relevant stimulus information in temporarily and spatially distributed activity across neurons were termed dynamic population coding \citep{sreenivasan2014revisiting}.\\
%A prominent theory in line with non-stable representations is that cognitive processes evolve through different so-called \textit{brain states} as dynamic neural trajectories \citep{buonomano2009state}.
%These trajectories emerge by conceptualizing neural responses as vectors in an N-dimensional space that evolves over time:
%The axis of the high-dimensional space correspond to measurements, such as raw or transformed data from M/EEG sensors or \gls{fMRI} voxels.
%The neural trajectories emerge by tracing the activity on these axes over multiple time points.
%
%%\begin{figure}
%%	\centering
%%	\includegraphics[width=0.5\textwidth]{memento/dynamic_coding.png}
%%	\caption[The dynamic coding framework]{A schematic illustration of dynamic coding in neural populations. The left side depicts a population of three neurons (squares), and their activity (color) over six time points time in response to two visual stimuli. The neural code in this population is not stationary, but dynamic: Each stimulus evokes a distinct response, but within this response, each time point has a different pattern of neural activity, too. By conceptualizing activity patterns over time as vectors, these patterns can be visualized in a high-dimensional space (right). This space has as many dimensions as there are measurement sites (e.g., neurons, sensors, voxels, or electrodes). Dynamic population codes create trajectories through this space. Adapted from \citep[][Fig. 2]{meyers2018dynamic}.}
%%	\label{fig:dynamiccoding}
%%\end{figure}
%
%To bridge the gap to theories of activity silent states, these neural responses are not solely determined by external stimuli, but also internal state changes of the network, such as the strength of synaptic connections, or excitatory or inhibitory influences from other networks \citep{buonomano2009state}.
%Distinct neural states were often hypothesized to reflect steps in mental processing \citep[e.g.,][]{seidemann1996simultaneously}.
%\citet{muhle2021hierarchy}, for example, proposed a hierarchy of functional states in working memory, with items relevant for a pending decision in the most active state, and items relevant only for later use in latent states.
%But studies were also able to extract external stimulus information from the trajectories of transient states, for example odors \citep{mazor2005transient}.
%The study of these neural states and their transition has emerged as its own field, employing multivariate methods such as decoding across neural populations, hidden Markov models (HMMs) to model ensemble activity as a sequence of constantly shifting neural states \citep{rainer2000neural}, or functional alignment \citep{haxby2011common} to find common representations across idiosyncratic neural responses.
%In parallel, dimensionality-reduction gained popularity in analyzing neural population data \citep{cunningham2014dimensionality}.
%The underlying assumption to this approach is that the measured neural activity is too complex compared with the few relevant task or stimulus aspects that require encoding, and that the neural signal of interest must be embedded in a subspace of the high-dimensional neural space.
%\citet{santhanam2009factor} employed a decoding approach based on factor-analysis to reduce noise in high-dimensional neural recordings to improve the performance of neural prostheses, which otherwise suffered from the neural variability introduced by changes in attentional state or wakefulness.
%The term ``effective dimensionality'' of population activity emerged in the literature to identify shared components of collective dynamics that reflect task variables \citep{jazayeri2021interpreting}.
%\citet{murray2017stable} used \gls{pca} on electrophysiology recordings acquired from primate prefrontal cortex during a working memory task, and found a lower-dimensional subspace in the high-dimensional state space in which stimulus representations were stable across the cue and delay epochs.
%\citet{machens2010functional} likewise used \gls{pca} in a working memory task where non-human primates compared the frequency of two vibrations to identify a 6-dimensional subspace in which this tactile stimulus information is represented.
%\citet{gallego2017neural} proposed the neural manifold hypothesis.
%In this hypothesis, behavior is controlled by neural modes, patterns of neural population covariance.
%The high-dimensional neural space is confined into a lower dimensional ``manifold'' spanned by these neural modes.
%Dimensionality reduction methods would allow to identify the foundational neural modes that confine population activity, and allow the study of neural population dynamics.
%They further showed that the low dimensional spaces remained stable over multiple years by aligning the manifolds from separate measurements \citep{gallego2020long}.\\
Drawing insights from the partial overview of the literature in Chapter \ref{chap:k1}, methods to study working memory have shifted from traditional evoked potentials to computationally intensive processing.
The field has progressed from studying spiking activity in single cells to an implicit consensus that the neural representation of working memory during maintenance is high-dimensional or embedded in a high-dimensional space, time-varying, and spatiotemporal distributed.
But despite the wealth of theories and methodological approaches, the field has not yet converged on a full understanding of working memory.
Recent reviews have begun to point out where findings and standard views fall short to yet explain the mechanisms underpinning working memory \citep{nobre2022opening}, or highlight the inconsistencies in results across studies \citep{pavlov2022oscillatory}.
While studies traditionally focused on the involvement of prefrontal areas due to its involvement in executive control \citep[e.g.,][]{fuster1971neuron, funahashi1989mnemonic, miller1996neural}, many other brain areas have been shown to represent working memory items, too.
In a recent review, \citet{sreenivasan2019and} summarized how single-cell measurements, MEEG, and \gls{fMRI} acquisitions have found increased activity during working memory maintenance throughout the cortex and even some subcortical areas.
\citet{d2007cognitive} described working memory as an emergent property of functional interactions prefrontal areas and the rest of the brain as opposed to being localized to a single brain region.
And while an often proposed mechanism for the coordination of temporal and spatial population codes are brain oscillations in specific frequency bands \citet[e.g.,][]{roux2014working}, %for example concluded that gamma and alpha band oscillations of groups of neurons are generically involved in the maintenance of sensory-spatial working memory items.
a recent systematic review by \citet{pavlov2022oscillatory} highlighted a prominent lack of agreement across results in experimental studies that reported observations of brain oscillations in working memory tasks.\\
%Decoding approaches can find multiple different working memory items simultaneously in the same neural population \citep{rigotti2013importance}, and in various brain regions \citep{curtis2010beyond}.
In light of variable and even conflicting findings, \citet{nobre2022opening} called for sustained open-mindedness and creativity in researching working memory in a recent reflective piece.
In this exploratory spirit, I conducted the simulations and analyses that will be described in this chapter.
They connect to the works of \citet{murray2017stable}, \citet{machens2010functional}, \citet{rigotti2013importance} and others who employed dimensionality reduction methods perform statistical decomposition in a high dimensional ``native neural space'' to
project the axes of a task space into subspaces of the native neural space (see Figure \ref{fig:trajectories}).
They do not confine the neural signal to a specific brain area, but attempt to find neural signatures of decision-relevant working memory items across the cortex.
Central to the analyses is the attempt to use a method for functional alignment and dimensionality reduction on \gls{meg} data.



%\begin{figure}
%	\centering
%	\includegraphics[width=0.5\textwidth]{memento/neural_trajectories_redone.png}
%	\caption[Neural trajectories through state-spaces]{An schematic illustration of neural trajectories through a state-space for two decision alternatives (blue) and two response alternatives (red), for two tasks in a delayed-response-mapping paradigm. Assigning meaning to the axes of the state-space may yield insights about brain states in different experiment conditions. Adapted from Jocham, Krauel, Hanke (unpublished)}
%	\label{fig:trajectories}
%\end{figure}




%The decoding approach, generally speaking, works by identifying patterns that correspond to the experimental task or stimulus in population signals with machine-learning classifiers.


\section{Magnetoencephalograpy (MEG)}

The plurality of analyses in the upcoming sections warrants a more detailed overview of the basic properties, strengths, and weaknesses of the underlying data, stemming from magnetoencephalography acquisitions.
\gls{meg} is a neuroimaging method to record neural activity from magnetic flux, the total magnetic field passing though a given area.
It is measured on the surface of the head with sensitive detectors, so-called Superconducting Quantum Interference Devices (SQUIDs).
The electric currents that give rise to these magnetic fields stem from synchronous inhibitory or excitatory postsynaptic potentials in parallel pyramidal cells in the cortex, mostly those cells in the walls of the cortical sulci.
As the potential differences between soma and axons of neurons form magnetic dipoles, and these cells are perpendicular to the cortical sheet of the gray matter and thus similarly oriented, measurable magnetic fields emerge at the sculp when aggregated across a neuronal population.
The resulting cortical magnetic fields are on the order of $100$-$500$ femtotesla (fT) \citep{hari2017primer}.
Compared to the environmental noise, this is a very weak magnetic field.
Therefore, in addition to active and passive shielding from magnetic or electronic inferences, liquid helium in the MEG device cools the SQUIDs to -269°C and allows them to be superconductive.
In this state, a superconducting coil in the SQUID can amplify the magnetic flux, which is picked up by a nearby pickup coil, also called flux transformer.
Three main types of flux transformers exist: magnetometer, axial gradiometer, and planar gradiometer, each with a different sensitivity profile.
An MEG device can contain different types of flux transformers and the total number of flux transformers is the amount of channels of the MEG system.
The MEG analysis consists of interpreting the resulting topographies given the employed flux transformer.
Typically, MEG devices contain around 150-300 sensors arranged in a helmet-shaped array that cover the whole human scalp at a frequency of 1000-5000Hz.
The resulting temporal resolution is high compared to neuroimaging methods such as \gls{fMRI} or \gls{pet}.
This makes it a highly suitable modality to study cognitive processes that occur in the range of milliseconds.
The spatial resolution is, however, inferior to the spatial resolution that can be achieved with \gls{fMRI}.
This is aggravated further by the superposition principle of the magnetic field, stating that fields arising from several currents are linear sums of fields generated by each single current.
Thus, the measured signal at the sculp is a mix of multiple magnetic fields, stemming from different sources within and outside the brain that are difficult to disentangle \citep{hari2017primer}.
Data are said to be in ``sensor space'' when read out from the sensors, and in ``source space'' when its cortical sources have been estimated.
All analyses conducted on \gls{meg} data in this chapter were carried out in sensor space.



\section{The MEMENTO study}


The data basis of the upcoming analyses stems from the memento study.
It investigated how decision-relevant information is retained in working memory using a decision making task with concurrent \gls{meg} acquisition.
The data was acquired as part of the SFB 779 project B16N (``offline value representations in sequential decision making'') at the Otto-von-Guericke University Magdeburg in 2016 by \citet{kaiser}, and has not been previously published.
%The following study overview adheres to the COBIDAS MEEG reporting guidelines for reproducible MEEG research \citep{pernet2020issues}.


\subsection{Participants}

$N = 22$ right-handed, healthy participants with normal or corrected-to-normal vision were recruited at the Center for Behavioral Brain Sciences and on the university campus Magdeburg.
Their mean age was 26 years, and 10 participants were male.
Handedness was assessed with the Edinburgh Handedness Inventory \citep{oldfield1971assessment}.
Participants gave their informed consent to participate in the study, and received a base monetary compensation in the order of 8€ per hour with a performance-dependent bonus of up to 3€.
Ethics approval was obtained from the University Clinic Magdeburg.

\subsection{Experimental design}

The experiment consisted of 510 trials, grouped into 5 blocks with a variable break in between.
Each trial required a decision between one of two stimulus options, presented as gabor patches on the left and right side of the screen (\cref{fig:memento_trial}).
Importantly, stimulus options were presented in succession, with a delay period through which the decision-relevant properties of the first stimulus had to be retained in working memory.
The number of stripes in the gabor patch encoded the reward magnitude (either 0.5, 1, 2, or 4 points), and the angle of stripes encoded reward probability (either 10\%, 20\%, 40\%, or 80\%): The more stripes, the higher the reward, and the larger the angle, the higher the probability.
Participants learned these associations in a tutorial prior to the experiment (\cref{fig:memento_tutorial}), and \cref{fig:memento_stim} provides an overview of stimuli.
Stimulus combinations and sequences were selected to fulfill the following requirements:
\begin{enumerate}
	\item To prevent subjects from forming a decision already in the delay period \citep{curtis2010beyond}, the left stimulus never contained the best or worst combination of properties (see Figure \ref{fig:memento_stim})
	\item In the first option, all magnitude and probability options are shown equally often
	\item No more than three repetitions of the same probability and magnitude combination occur in the left stimulus
	\item First and second stimulus are never identical
	\item The expected values of the left and right stimulus were balanced over the course of the experiment such that left and right options yielded the same reward on average
\end{enumerate}
Based on these requirements, a fixed reward schedule was generated, and all subjects underwent the same sequence of stimulus combinations.
In approximately half of the trials one of the two stimulus options was clearly favorable, and both its properties (magnitude and probability) were either equal to or greater than that of the other option.
These trials consequently required no integration of stimulus properties to assess which option's value is higher.
In a subset of those trials, called ``no-brainer'' in all following analyses (roughly 10\% of all trials), both magnitude and probability were strictly greater in one of the two options, posing the easiest decision form.
In standard trials, however, stimulus properties were differentially advantageous for the two options.\\
Each trial started with a fixation cross (1000-1900ms, jittered), followed by the first stimulus on the left side of the screen (700ms), a 2000ms delay period through which a central ``or'' was presented, the second stimulus option on the right side (700ms), and a feedback screen.
Once the second stimulus option was displayed, participants chose the left or right option via a button press with the right or left index finger, respectively.
The feedback screen showed both options side by side with a frame around the chosen option, and revealed which option(s) had been rewarded via color coding (red: unrewarded, green: rewarded).
If a decision was not made within 5 seconds, the trial was aborted and participants saw a message to respond faster.
A progress bar at the bottom of the screen tracked gains over time, resulting in a bonus payment whenever it hit a gold target line.
Participants were instructed to maximize their gains and to respond as fast as possible.
Stimulus presentation was controlled by Psychtoolbox \citep{kleiner2007s} running on Matlab 2012b (The Mathworks Company, Natick, MA).
All stimuli were presented on a grey background with a contrast optimized for the MEG recording chamber.
%A photodiode, taped onto the stimulation screen, was used to determine visual stimulus onsets.
%\todo{Add delay}
In total, the experiment lasted approximately 60 minutes.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{memento/memento-stimuli.png}
	\caption[Stimulus overview]{Stimulus overview: Gabor patches varied in the number of stripes and their angle, encoding magnitude and probability, respectively. While all presented stimuli were learned in the tutorial, red letters indicate the selection of nine stimulus types used for the left option in the actual experiment. Trials without this annotation were only used intermittent as the right stimulus option to balance the overall expected value between the left and right stimulus option over the course of the experiment.}
	\label{fig:memento_stim}
\end{figure}

\begin{figure}
	\begin{subfigure}{.54\textwidth}
	\includegraphics[width=\textwidth]{memento/memento_tutorial.png}
	\caption{Schematic overview of the tutorial.}
	\label{fig:memento_tutorial}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
	\includegraphics[width=\textwidth]{memento/memento_experiment.png}
	\caption{Schematic overview of a single trial.}
	\label{fig:memento_trial}
\end{subfigure}
	\caption[Memento: Tutorial and trial overview]{Schematic overview of the tutorial (\ref{fig:memento_tutorial}) and a single trial in the experiment (\ref{fig:memento_trial}).
	The tutorial was split in an active and a passive part.
	In the passive part, participants were presented with a stimulus and how its properties translated to reward magnitude and probability. All possible magnitude and probability combinations were presented twice, and participants controlled the pace. The active part then tested participants' knowledge by presenting a stimulus without the annotation. Based on a text prompt, participants had to report its associated magnitude or probability via button press. They received feedback with a green colored stimulus for a correct response, or, for an incorrect response, a red colored stimulus together with a report of their response versus the true response.
	}
\label{fig:memento}
\end{figure}


\subsection{MEG acquisition}

Following a metal test, MEG data were acquired on an Elekta Neuromag TRIUX System with internal helium recycler and 306 sensors (204 planar gradiometers and 102 magnetometers) in a magnetically shielded room.
Participants were instructed to take a comfortable seating position and sit as still as possible.
The experiment was presented on a screen in a distance of one meter from the sitting participants via a projector with a refresh rate of 60 Hz located outside the MEG recording chamber.
Additional sensors captured confounding biological signals:
Lateral and vertical eye movements and blinks were captured using \gls{eog} surface electrodes on the tori supra- and infraorbitalis and next to the external canthi.
Heartbeat artifacts were recorded with an electrocardiogram (ECG).
Head position indicator coils captured the participants' head movements.
Behavioral responses were registered using an MEG compatible keyboard.
The neural data was recorded at a sampling rate of 1000Hz and active internal shielding (IAS).


\section{Data preparation and analysis}

The code used for preprocessing and analysis was bundled into a publicly available Python package \texttt{pymento\_meg} and is available from GitHub\footnote{\url{https://github.com/adswa/pymento_meg}} and \url{PyPi.org}.
Underlying software packages are listed in Table \ref{tab:software} and were bundled into a publicly available Singularity image (TODO).
\begin{center}
\begin{table}
	\begin{tabular}{ l l l }
		\hline
		Software	& Version 	& Citation \\ \hline
		autoreject 	& 0.4.2 	& \citet{jas2017autoreject} \\
		brainiak 	& custom fork based on v.011 & \citet{brainiak} \\
		imbalanced-learn & 0.10.1 & \citet{JMLR:v18:16-365} \\
		matplotlib 	& 3.7.1 	& \citet{Hunter2007} \\
		meegkit 	& 0.1.3 	& \citet{barascud2022} \\
		mne-bids 	& 0.12 		&  \citet{Appelhoff2019} \\
		mne-python 	& 1.4.0		& \citet{Gramfort_MEG_and_EEG_2013} \\
		pandas 		& 2.0.2 	& \citet{The_pandas_development_team_pandas-dev_pandas_Pandas} \\
		scikit-learn & 1.0	 	& \citet{scikit-learn} \\
		scipy 		& 1.10.1 	& \citet{2020SciPy-NMeth} \\
		seaborn 	& 0.12.2 	& \citet{Waskom2021}
	\end{tabular}
	\caption[Overview of software packages]{Overview of internally employed software packages, their versions, and citations.}
	\label{tab:software}
\end{table}
\end{center}

\subsection{Data preparation}

Prior to preprocessing, raw data were restructured to \gls{BIDS} format (v1.4.0).
Several properties of the project made this challenging.
As the acquisition was done in a different institution several years back by \citet{kaiser} and the data had moved locations, crucial metadata was missing from the project files, either because it was lost or not recorded in the first place.
Some information, such as metadata files from the acquisition machine, was acquired post-hoc by emailing the facility in Magdeburg.
Other information, such as some participants' ages or raw data from the handedness acquisitions could not be found.
Furthermore, as \gls{meg} data was acquired on an Elekta Neuromag machine, previous ``raw'' data former analyses were based on was preprocessed in-scanner with Neuromag's propietary MaxFilter\textsuperscript{TM} for motion correction and denoising.
Data that is preprocessed with such a proprietary tool is not an ideal data analysis basis as it can not be easily recomputed without access to the original acquisition machine, which would make a fundamental processing step nontransparent.
However, as only preprocessed files were used previously, it went unnoticed that one of the pristine raw \gls{meg} data files was not copied over from the acquisition machine to the project archive.
With the help of the original authors and a contact at the acquisition facility in Magdeburg it was possible to restore this file from internal archives.
Finally, as the experiment was Matlab-based, behavioral log files were written to proprietary .mat files.
During the transformation to \gls{BIDS}, these log files were transformed into the open TSV format (see Figure \ref{fig:BIDS}).

\subsection{Preprocessing}
\label{preprocessing}

An overview of preprocessing steps is shown in Figure \ref{fig:preproc}.
The following paragraphs outline individual steps in detail.

\begin{figure}
	\centering
	\includegraphics[width=1.\textwidth]{memento/preprocessing_overview.png}
	\caption[Preprocessing overview]{Preprocessing Overview}
	\label{fig:preproc}
\end{figure}

\textbf{Signal Space Separation} As the first step of preprocessing, the spatiotemporal extension of the \gls{SSS} method \citep{taulu2005presentation}, \textit{\gls{tSSS}} \citep{taulu2006spatiotemporal}, was applied, as is common for recordings on Neuromag MEG systems with active internal shielding.
\gls{SSS} and \gls{tSSS} remove strong interference from external noise sources and sources within the body itself from the MEG signal.
The methods can be applied on whole-scalp multichannel data when the precise sensor calibrations are known, and were initially developed as the proprietary MaxFilter\textsuperscript{TM} algorithm by Elekta Neuromag.
To this end, Neuromag systems provide a cross-talk compensation and fine calibration file which reduces interference between their co-located magnetometer and paired gradiometer sensor units and encodes site-specific information about sensor orientation and calibration, respectively.
Based on the Maxwell equations and the geometry of the sensor array, \gls{SSS} decomposes \gls{meg} signals ($\phi$) into elementary magnetic fields from sources within the sensor helmet (the \textit{internal} subspace with the signal of interest) and into an orthogonal set for fields arising from sources outside (the \textit{external} subspace with interference sources) \citep{taulu2005presentation}.
To this end, it transforms the $N=306$-dimensional signal vector into a lower-dimensional, linearly independent subspace that spans all measurable signals, the ``SSS basis'' $S$.
Its dimensionality is dependent on two user-defined truncation values $L_{in}$ and $L_{out}$, which correspond to the highest possible frequencies in the internal and external subspace.
For Elekta systems with $N=306$ channels, optimal values are $L_{in}=8$ and $L_{out}=3$ \citep{taulu2005sss}, resulting in
$n=((L_{in}+1)^2 -1) + ((L_{out}+1)^2 - 1) = 95$ dimensions (80 internal, 15 external) \citep{taulu2005presentation}.
With $n \ll N$, $\phi$ can be uniquely decomposed into internal and external subspaces $S_{in}$ and $S_{out}$, which contain the biomagnetic signal and arbitrary external interference, respectively.

%\begin{equation}
%	\begin{aligned}
%		  \phi = \sum_{l=1}^{\inf}\sum_{m=-l}^{l} \alpha_{lm} a_{lm} + \sum_{l=1}^{L_{out}}\sum_{m=-l}^{1}\beta_{lm}b_{lm}
%		\end{aligned}
%\label{eq:sss}
%\end{equation}


%\begin{equation}
%	\begin{aligned}
%		\phi &= S_x = [S_{in} S_{out}] \begin{bmatrix}
%			x_{in} \\
%			x_{out}
%		\end{bmatrix}
%	\end{aligned}
%	\label{eq:sss1}
%\end{equation}





\begin{equation}
	\begin{aligned}
			\phi &= S_x = [S_{in} S_{out}] \begin{bmatrix}
			x_{in} \\
			x_{out}
		\end{bmatrix} = \phi_{in} + \phi_{out}
	\end{aligned}
	\label{eq:sss}
\end{equation}

As the internal and external subspaces are provably linearly independent, brain signals are then reconstructed by retaining only sources inside the helmet, thus excluding external inferences.


\begin{equation}
	\begin{aligned}
    \hat{x} =
\begin{bmatrix}
	\hat{x}_{in} \\
	\hat{x}_{out}
\end{bmatrix}
= S^{\dagger}\phi \\
\hat{\phi}_{in} = S_{in}\hat{x}_{in}\\
	\end{aligned}
	\label{eq:sss}
\end{equation}


According to \citet{taulu2006spatiotemporal}, \gls{SSS} can separate brain signals from sources >0.5m away, suppressing external interference by a factor >100.
The spatiotemporal extension \gls{tSSS} can further detect inferences from closer sources such as stimulators or pacemakers.
These are estimated based on the fact that their strength typically exceeds that of sensor noise and they thus, unlike brain signal, leak into both the internal and external part of the \gls{SSS} reconstruction.
After detecting components with a high temporal correlation between the external and internal subspaces, \gls{tSSS} removes close-by artifacts by projecting the components common to the internal and external subspace out of the internal subspace.
In the absence of nearby artifacts, \gls{tSSS} reduces to \gls{SSS} \citep{taulu2009removal}.
\gls{tSSS} was implemented using mne-python's open source implementation \texttt{maxwell\_filter()} with a chunk duration of 10 seconds and a correlation threshold of at least $0.98$.
Prior to \gls{tSSS}, bad channels were detected and annotated automatically in order to prevent bad channel noise from spreading.
To compensate for head movements, measurements from the head position indicator coils were used to estimate subject motion, and motion correction was then performed as part of the \gls{tSSS} procedure: As the signal representation in the \gls{SSS} basis is device independent, internal data can simply be transformed to a sensor array corresponding to the average head position.
Similarly, formerly bad channels have also been effectively repaired by the procedure.
After \gls{tSSS}, gradiometers and magnetometers contain highly similar information and have an altered inter-channel correlation structure because they were reconstructed from a common 80-dimensional subspace \citep{garces2017choice}.


\textbf{ZAPLine filtering} After \gls{tSSS}, visual inspection revealed that minor spectral artifacts remained in the data, among them power line noise at 50Hz and a spectral peak at 60Hz, likely originating from the presentation screen's refresh rate.
ZAPline filtering \citep{de2020zapline} was performed to remove them.
Figures \ref{fig:prezap} and \ref{fig:postzap} show power spectral density plots of the signal before and after applying ZAPline filters.


\begin{figure}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=\textwidth]{memento/psd_pre_zapline.png}
		\caption{Power spectral density before ZAPline filtering}
		\label{fig:prezap}
	\end{subfigure}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=\textwidth]{memento/psd_post_zapline.png}
		\caption{Power spectral density after ZAPline filtering}
		\label{fig:postzap}
	\end{subfigure}
	\caption[Power spectral density before and after ZAPLine filtering]{Power spectral density of all MEG channels
		from a single subject before (\ref{fig:prezap}) and after (\ref{fig:postzap}) ZAPLine filtering.
		Two spikes at 50Hz (power-line frequency) and 60Hz (likely an artifact of the stimulus presentation) are markedly reduced afterwards.
	}
	\label{fig:zapline_psd}
\end{figure}

\textbf{Filtering} Next, data were first low-pass filtered with a 100Hz lowpass FIR filter to constrain it into a frequency range of interest.
The filter properties are reported in \ref{fig:preproc} and a visualization is in \ref{fig:filter}.

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{memento/filter_properties_100hz.png}
	\caption[Lowpass filter properties]{Lowpass filter properties}
	\label{fig:filter}
\end{figure}


\textbf{Independent component analysis} As eye movements, eye blinks, heart beats, and facial muscle contractions survive \gls{tSSS}, \gls{ica} was used to detect and remove these artifacts.
% The slow drifts are problematic because they reduce the independence of the assumed-to-be-independent sources (e.g., during a slow upward drift, the neural, heartbeat, blink, and other muscular sources will all tend to have higher values), making it harder for the algorithm to find an accurate solution. A high-pass filter with 1 Hz cutoff frequency is recommended. However, because filtering is a linear operation, the ICA solution found from the filtered signal can be applied to the unfiltered signal https://ieeexplore.ieee.org/document/7319296/
As \gls{ica} is sensitive to low-frequency drifts \citep{winkler2015ICA}, the data were first processed with a temporary 1Hz highpass filter. %one-pass, zero-phase, non-causal highpass filter (firwin method, Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation, a lower passband edge of 1.00, lower transition bandwidth of 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz), and a filter length of 3301 samples (3.301 s)).
ICA can further be sensitive to bad segments in the recording.
Therefore, data were temporarily epoched into 5 second splits from the onset of the fixation cross.
\texttt{autoreject}, an algorithm to reject and repair bad trials as well as to estimate bad sensors on a per-trial basis, was then used on the first 200 of these epochs to estimate the noise level and compute rejection thresholds.
Afterwards, FastICA \citep{hyvarinen1999fast} was used to decompose the signal of all epochs below these rejection thresholds into 45 independent components.
For most subjects, components corresponding to ECG activity were identified using cross-trial phase statistics \citep{dammers2008integration} with an automatically computed threshold of 0.16 for the Kuiper statistic, and \gls{eog} related components were found using Pearson correlation.
For one subject, however, the ECG channel was flat, and components were manually selected.
Afterwards, the \gls{ica} solution was applied to the continuous recording and detected ECG and \gls{eog} components were zeroed out.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{memento/average_epoch_cleaned.png}
	\caption[Average neural signal over the trial course]{Neural recordings from a single exemplary subject.
		Shown is the average of cleaned epochs with a length of 5 seconds from left stimulus onset, corresponding to a trial from first visual stimulation until
		1600ms after the offset of the second stimulus.}
	\label{fig:cleanepoch}
\end{figure}

\textbf{Bad segment rejection and repair} As a final step, the continous recording was chunked into epochs of varying length depending on analysis, and \texttt{autoreject} was used to detect and repair or drop bad epochs.
Figure \ref{fig:preproc} summarizes the preprocessing steps.
Figure \ref{fig:cleanepoch} shows an average of cleaned 5 second epochs for a single subject.



\subsection{Analysis prerequisites}
\label{decoding}

Many of the upcoming analyses are rooted in machine-learning methodology.
This section introduces relevant concepts, and summarizes common elements across analysis.
I set up the machine-learning based analyses either purely in the standard library \textit{scikit-learn} (\ref{behavioral-analysis}, behavioral analysis), using functionality from \textit{mne-python} that builds up on scikit-learn for MEEG applications (\ref{temporal-generalization-analysis}, temporal generalization analysis), or by implementing scikit-learn compatible custom estimators and scorers myself (\ref{decoding-analysis}, decoding analyses).\\
Even if upcoming analyses extend scikit-learn's capabilities, the framework's terminology remains central to describe the analysis setup in depth:
All analyses I conducted are classification problems, belonging to the family of supervised learning.
In these analyses, the goal is to predict a discrete target variable $y$ from an input data matrix $X$ with $n$ observations, for example, if a participant performs a left or right response (target) based on the stimulus properties (features) in each trial.
In scikit-learn's terms, an \textit{estimator} is what performs this prediction.
A \textit{scorer} is a method that evaluates an estimator's predictions on a given dataset, such as computing the accuracy from predicted and actual target labels.
A \textit{transformer} is an object that alters the input data, for example by cleaning, reducing, or expanding it, or by generating features from it.
Fundamental to evaluating machine-learning applications are train-test data splits.
These partition the available data into a set used to train the estimator, and a set yet unknown to the estimator, used to test how well its predictions generalize to unseen data.
The latter gets evaluated with a scorer.
I set this up within a k-fold cross-validation framework, which repeats model fitting and predictions with test and train splits over $k$ different partitions of data such that each partition $k$ is used as a test set once.
The final model evaluation is based on an average of the prediction performance in each fold, reducing the variance of the model performance estimate.
In classification problems with an unequal amount of target classes I used a stratified k-fold cross-validation paradigm, which creates splits such that the relative distribution of class labels in its split matches that in the full dataset approximately.\\
If testing data leaks into the training data, performance estimates get overly optimistic as the model is partially trained on the data it will be evaluated on.
To avoid this common pitfall, all machine-learning analyses were set up as a so-called \textit{pipelines}, sequences of transformers or estimators that chain processing steps while internally ensuring that test and train data are kept strictly separate.
Additionally, pipelines ensure consistent preprocessing by applying transformers to training and test data.
A transformer common to pipelines in all analyses was a \texttt{StandardScaler()}, transforming data to have zero mean and unit variance.
The final estimator common to all pipelines was a logistic regression classifier with L2 regularization and a \texttt{liblinear} solver.
The L2 regularizer of this estimator assumes that all features are centered around zero or have variance in the same order \citep{scikit-learn-scaler}.
Even in \gls{meg} data in which different sensor types typically have different scales, this prerequisite holds due to the combination of \gls{SSS} and scaling.
Internally, the \texttt{liblinear} solver uses a coordinate descent (CD) algorithm, which extends binary classifications problems to a multinomial case by decomposing the optimization problem into several one-vs-rest problems \citep{scikit-learn-liblinear}.\\
Where the analysis is a decoding analysis, a model predicts a target, such as stimulus identity, based on considering the relationships between multiple \gls{meg} channels.



% From Grootswagers et al. (2017): MVPA for MEG/EEG The term “multivariate pattern analysis” (or MVPA) encompasses a diverse set of methods for analyzing neuroimaging data. The common element that unites these approaches is that they take into account the relationships between multiple variables (e.g., voxels in fMRI or channels in MEG/EEG), instead of treating them as independent and measuring relative activation strengths. The term “decoding” refers to the prediction of a model from the data (“encoding” approaches do the reverse, predicting the data from the model, reviewed in Naselaris, Kay, Nishimoto, \& Gallant, 2011; see also, e.g., Ding \& Simon, 2012, for an example of encoding models for MEG). The most common application of decoding in cognitive neu-roscience is the use of machine learning classifiers (e.g., correlation classifiers (Haxby et al., 2001) or discriminant classifiers (Carlson et al., 2003; Cox \& Savoy, 2003) to identify patterns in neuroimaging data, which correspond to the experimental task or stimulus. The most popular applications of MVPA are decoding (for recent reviews on fMRI decoding, see e.g., Haynes, 2015; Pereira et al., 2009)and, more recently, representational similarity analysis (RSA: Kriegeskorte \& Kievit, 2013).

%\subsection{machine learning concepts in scikit-learn}
%\texttt{y} is the \textit{target}, also called \texttt{outputs}, \texttt{responses}, \texttt{label} or \texttt{ground truth}.¸ Its the \texttt{dependent variable} in supervised learning, passed to an \texttt{estimator}'s fit method as y.
%\texttt{X} is the observed data. Its number of rows are the number of \texttt{samples}.
%\texttt{features} are the individual elements of a vector representing a sample. In a data matrix, features are represented as columns. Elsewhere features are also known as attributes, predictors, regressors, or independent variables.
%\texttt{samples} typically denote a single feature vector. Elsewhere, a sample is called an instance, data point, or observation. \texttt{n\_samples} indicates the number of samples in a dataset, being the number of rows in a data array \texttt{X}
%A \texttt{classifier} is a predictor with a finite set of discrete possible output values, and must implement "fit", "predict", and "score" methods, and could implement "decision\_function", "predict\_proba" and "predict\_log\_proba" methods.
%An \texttt{estimator} is an object that manages the estimation and decoding of a model. An estimators fit method takes samples X, target y, and sample properties (e.g., weights). Once fitted, the "predict\_proba" method can return probability estimates for each class from some input data X. A \texttt{score} is a method that evaluates predictions on a given dataset, returning a single
%\texttt{evaluation metrics} accept a ground truth and a prediction (e.g., output of predict, predict\_proba, etc)
%\texttt{Transformers} (or \texttt{transforms}) can clean, reduce, expand, or generate feature representation. is
%\texttt{Pipelines} sequentially chain transforms with a final estimator, and reduce the possibilities of forgetting transformations that could lead to inconsistent preprocessing applications between training and testing data and of leaking test data into training data. The purpose of a pipeline is to assemble several steps that can be cross-validated together whiule



\subsection{Behavioral analysis}
\label{behavioral-analysis}

As a first step, I conducted several analyses on behavioral data to check if participants' behavior matched the task demands and to inform further processing.
For the former, I conducted analyses regarding participants' performance.
One measure to evaluate performance in the experiment is the gain that participants achieved at the end of the last trial.
It is the sum of rewards accumulated over all trials, and should be higher the better participants judged stimulus values.
On average, participants gained $G=349.5$ points in the experiment (range = $[300.5, 393]$; top boxplot in Figure \ref{fig:gains}).
To assess whether this exceeds chance level performance, I conducted a simulation of $N=10.000$ experiments with random choice behavior (bottom boxplot in Figure \ref{fig:gains}), confirming that all participants performed significantly better than chance at the 95\% confidence level (dotted line in Figure \ref{fig:gains}).
A different measure of performance is reaction time.
The average reaction time pooled across participants was $\mu_{RT}=0.93$s.
Contrary to the expectation that no-brainer trials would pose easier decisions, the difference in average reaction times for no-brainer trials compared to standard trials was negligent ( $\mu_{RTnobrainer}=0.91$s compared to  $\mu_{RTstandard}=0.94$s; Figure \ref{fig:reactiontimes}).
An inspection of average reaction times per subject revealed a small subset of slow participants with average reaction times exceeding 1.3s (Figures \ref{fig:avgreactions}, \ref{fig:avgreactionsnobrain}).\\
To inform further processing, I investigated which stimulus properties influenced participants' behavior.
The stimulus properties relevant for the eventual behavioral choice in each trial were reward magnitude (stripe frequency), reward probability (stripe orientation ), and their combination (expected value).
I assessed their relative influence on choice by two means: First, by simulating different strategies by which participants could use these properties, and comparing them against the actual gains.
I created five different strategies: Pure probability- or magnitude-based strategies (choice fell on which ever stimulus had a higher value, and reduced to random choice where values were equal), primarily probability- or magnitude-based strategies (choice fell on which ever stimulus had a higher value, and in cases where values were equal, it fell on which ever stimulus had a higher value on the other property, akin to a sequential evaluation process), and a expected-value-based strategy (choice fell on the stimulus with the higher expected value, and reduced to random choice where expected values were equal).
These simulations revealed that strategies based solely on either magnitude or probability yield outcomes that are predominantly worse than participants' actual gains, bearing a single outlier.
The strategy following expected value was the best, yielding predominantly higher gains than participants' actual gains, bearing -- again -- one outlier whose performance exceeded even that of 95\% of simulated experiments with the expected value strategy.
The two sequential strategies yielded a single, deterministic outcome that was close to the group average.
The second mean to assess which stimulus properties influenced choice was a stratified 10-fold cross-validated logistic regression analysis of left and right stimulus characteristics on choice for each subject.
The model included six predictors (magnitude and probability of the left and right option, and expected value calculated from demeaned magnitude and demeaned probability) and an intercept.
The analysis was constructed as a classification analysis with scikit-learn, using a L2 regularized logistic regression estimator.
Thus, the model estimated beta coefficients on a training set, and tested their predictive accuracy in a test set.
For each subject, I calculated the average accuracy over folds as a measure of model quality, and normalized beta coefficients within range $[0, 1]$ to assess their relative influence on choice behavior (Figure \ref{fig:logregbehavior}).
The average model accuracy was $0.83$ (std$=0.06$, range = $[0.68, 0.93]$).
The Spearman rank correlation between experiment performance (total gain) and model accuracy was $r=0.53$ ($p=0.012$), indicating that well-detectable influence of stimulus properties on choice was associated with higher gains in the experiment.
Across participants, reward probability had the highest relative influence on choice behavior, followed by reward magnitude, and the influence of left stimulus properties was slightly higher than that of right stimulus properties.
On the level of individual subjects this pattern held in all but one participant for whom magnitude had a higher influence than probability, and right stimulus probabilities had a higher influence than left stimulus probabilities.\\
Overall, these behavioral results confirm that participants acted according to task demands.
They further provide evidence that a majority of participants acted according to the strategy modeling decision as a sequential evaluation of probability and then magnitude (Figure \ref{fig:strategy-prob-first}).


%\begin{figure}
%	\includegraphics[width=1.\textwidth]{memento/behavior/average_gains.png}
%	\caption[Simulated versus actual experiment gains]{Participants gains in the experiment (top boxplot). The bottom boxplots is a simulation of the experiment (N=10000) with random choice behavior. Even the outlier at the bottom end of the actual results exceeds the results of more than 95\% of simulated experiments (dotted line)}
%	\label{fig:gains}
%\end{figure}


\begin{figure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/average_gains.png}
		\caption{Random choice}
		\label{fig:gains}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/gain-simulation-magnitude.png}
		\caption{Purely magnitude-based decision}
		\label{fig:strategy-prob}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/gain-simulation-probability.png}
		\caption{Purely probability-based decision}
		\label{fig:strategy-mag}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/gain-simulation-ev.png}
		\caption{Purely expected-value-based decision}
		\label{fig:strategy-ev}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/gain-simulation-magnitude-first.png}
		\caption{Primarily magnitude-based decision}
		\label{fig:strategy-mag-first}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/gain-simulation-probability-first.png}
		\caption{Primarily probability-based decision}
		\label{fig:strategy-prob-first}
	\end{subfigure}
	\caption[Experiment gains compared to different behavioral strategies]{Experiment gains compared to different behavioral strategies. Participants' actual gains in the experiment are always plotted in the top boxplot. The bottom boxplot is a simulation of the experiment ($N=10.000$) with different behavioral strategies. The red dotted line denotes the 95. percentile in the simulation data to allow for statistical comparisons at the 95\% confidence level. a) depicts simulated random choice behavior. Even the outlier at the bottom end of the actual results exceeds the results of more than 95\% of simulated experiments, indicating that all participants performed better than chance. b), c) and d) depict strategies that are purely based on higher magnitude, probability, or expected value, respectively, and reduce to random choice if the feature is equal across options. e) and f) depict deterministic decision strategies, based on sequentially evaluating first magnitude and then probability, or vice versa. Because no random element is involved, the average gain is identical over experiment repetitions.}
\end{figure}



\begin{figure}
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/average_reaction_times.png}
		\caption{Average reaction times}
		\label{fig:avgreactions}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{memento/behavior/average_no_brain_reaction_times.png}
		\caption{Average reaction times in no-brainer trials}
		\label{fig:avgreactionsnobrain}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[trim={0 0 0 2cm},clip, width=\textwidth]{memento/behavior/memento_aggregate_reaction_times_nobrainer.png}
		\caption{Distribution of reaction times}
		\label{fig:reactiontimes}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[trim={0 0 0 2cm},clip,width=\textwidth]{memento/behavior/average_property_importance.png}
		\caption{Parameter influence on choice}
		\label{fig:logregbehavior}
	\end{subfigure}

	\caption[Behavioral results]{\ref{fig:gains}\ref{fig:avgreactions} Average reaction times over trials for all subjects. \ref{fig:reactiontimes} Reaction times across trials and subjects for standard trials (orange) and trials with unambigous. \ref{fig:logregbehavior} Relative influence of stimulus properties on choice (left option)}
	\label{fig:behav}
\end{figure}

\subsection{Temporal generalization}
\label{temporal-generalization-analysis}

Extending the behavioral logistic regression classification, I performed several within-subject temporal generalization analyses to study the nature and progression of decision representation.
For this, I split the \gls{meg} data into separate epochs:
One set of epochs centered around the motor response with a length of 1000ms.
And one set of epochs locked to the first stimulus presentation, extending 3.4 seconds into the trial, until the end of the second stimulus presentation.
The first set of epochs was used to train multiple logistic regression estimators on \gls{meg} data in sensor space ($X=306 \times n\_samples$ features per trial) to classify the target $y$, left or right choice in each trial.
The second set of epochs was used as test data, to see when and how well a neural decision signal emerges over the trial course.
While the estimators where always trained on the entire training data, I manipulated the set of features in the test set to investigate if specific trial characteristics contributed differentially to a stronger representation.
For each analysis, I ran a permutation test with $N=10.000$ repetitions with shuffled targets to identify clusters of significant decoding accuracy.
To evaluate findings on the group level, I averaged all subjects' temporal generalization results, and bootstrapped a sample of each participants' permutation maps to estimate cluster reaching statistical significance following the approach proposed by \citet{stelzer2013statistical}.
Overall, I investigated the following manipulations:
First, I assessed choice decodability for trials with high versus medium versus low values in magnitude or probability in the first stimulus (Figure \ref{fig:temporal-generalization-actual}).
The underlying hypothesis for this analysis was if participants focus primarily on one of these properties, a high (or low) value in the first option might evoke a neural representation of a left (or right) choice already earlier in the trial, and allow an investigation if this neural code remains stable across the delay.
While these analyses revealed better decodability of eventual choice for high and low values compared to less informative medium values (only visible in raw accuracies), statistically significant clusters only emerged at the end of the trial where training time points and testing time points began to overlap.\\
Next, I adjusted the target definition in the test data.
Instead of decoding the actual choice at the end of the trial, I decoded a hypothetical target corresponding to strategies from Figures \ref{fig:strategy-mag} and \ref{fig:strategy-prob}:
If the first stimulus' magnitude (or probability, respectively) was high, I assumed the choice would be left, and vice versa.
While this analysis would not decode the actual ``choice'', it would decode a hypothesized preparation [????].
The results of this analyses reveal a stronger decodable cluster, in which neural code prior to the motor response generalize to trial parts during the first stimulus presentation and into the delay phase (Figure \ref{fig:temporal-generalization-hypothetical}). \\
Finally, I repeated this analysis taking into account the relative influence of stimulus property of each individual participant.
For this, I used the beta coefficients of the behavioral analysis to estimate the subjective choice $C$ according to the formula

\begin{equation}
	\begin{aligned}
	C = LMag * \beta_{LMag} + LProb * \beta_{LProb} + LEV * \beta_{LEV}
	\end{aligned}
\end{equation}

where $LMag$, and $LProb$ are the min-max scaled magnitude and probability values of the left stimulus option per trial, $LEV$ is the expected value calculated from demeaned magnitude and probability, similarly min-max scaled, and all $\beta$ values are beta coefficients obtained from the logistic regression analysis on behavioral data.
With this formula, the opposite ends of the value range of $C$ denote a tendency for the left and right stimulus option, respectively.
I extracted the lowest and highest quartile of trials as test data, assigning estimated choices as target labels.
The resulting decoding pattern mirrored that in Figure \ref{fig:temporal-generalization-actual}, revealing no clusters in the first stimulus presentation or delay period.\\
These temporal generalization results partially match findings in the literature.
[...]


\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_magnitude-high_averaged_actual_pval-mask.png}
		\caption{High magnitude trials (4 points)}
		\label{fig:generalization-mag-high-actual}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_probability-high_averaged_actual_pval-mask.png}
		\caption{High probability trials (80\% chance)}
		\label{fig:generalization-prob-high-actual}
	\end{subfigure}
	\hfill
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_magnitude-medium_averaged_actual_pval-mask.png}
		\caption{Medium magnitude trials}
		\label{fig:generalization-mag-medium-actual}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_probability-medium_averaged_actual_pval-mask.png}
		\caption{Medium probability trials}
		\label{fig:generalization-prob-medium-actual}
	\end{subfigure}
	\hfill
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_magnitude-low_averaged_actual_pval-mask.png}
		\caption{Low magnitude trials (0.5 points)}
		\label{fig:generalization-mag-low-actual}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_probability-low_averaged_actual_pval-mask.png}
		\caption{Low probability trials (10\% chance)}
		\label{fig:generalization-prob-low-actual}
	\end{subfigure}
	\caption[Temporal generalization: Eventual choice]{Temporal generalization results of decoding eventual choice (right option), overlaid with a permutation mask. The y-axes depict training time points, with 0 (horizontal line) being the motor response, negative values denoting trial samples prior to the motor response, and positive values denoting trial samples after the motor response. The x-axis depicts test data, covering the onset of the first stimulus until the offset of the second stimulus. Vertical lines illustrate trial structure. Only those parts of the plot that are not masked yielded accuracies that were higher than 95\% of accuracies obtained in $N=10.000$ permutations with shuffled training labels.}
	\label{fig:temporal-generalization-actual}
\end{figure}


\begin{figure}
%	\begin{subfigure}[c]{0.5\textwidth}
%		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_magnitude-high_averaged_hypothetical_pval-mask.png}
%		\caption{High magnitude trials (4 points)}
%		\label{fig:generalization-mag-high-hypothetical}
%	\end{subfigure}
%	\begin{subfigure}[c]{0.5\textwidth}
%		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_probability-high_averaged_hypothetical_pval-mask.png}
%		\caption{High probability trials (80\% chance)}
%		\label{fig:generalization-prob-high-hypothetical}
%	\end{subfigure}
%	\hfill
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_magnitude-low_averaged_hypothetical_pval-mask.png}
		\caption{Low magnitude trials (0.5 points)}
		\label{fig:generalization-mag-low-hypothetical}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=\textwidth]{memento/generalization/sub-group_generalization_probability-low_averaged_hypothetical_pval-mask.png}
		\caption{Low probability trials (10\% chance)}
		\label{fig:generalization-prob-low-hypothetical}
	\end{subfigure}
	\caption[Temporal generalization: Hypothetical choice]{Temporal generalization results of decoding hypothetical targets. Figure and analysis are similar to \ref{fig:temporal-generalization-actual}, but the decoding target (right option) was hypothetical, derived from whether the stimulus property in question was high or low in the first stimulus option.}
	\label{fig:temporal-generalization-hypothetical}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[trim={0 0.5cm 0 1.15cm},clip,width=0.5\textwidth]{memento/generalization/sub-group_generalization_all-trials_averaged_estimated_pval-mask.png}
	\caption[Temporal generalization: Estimated choice]{Temporal generalization results of decoding estimated targets. Figure and analysis are similar to \ref{fig:temporal-generalization-actual}, but the decoding target (right option) was estimated, derived from each participants' individual parameter influence.}
	\label{fig:temporal-generalization-estimated}
\end{figure}


\pagebreak

\subsection{Shared response modeling}
%improve
The previous temporal generalization analysis employed a continuous decoding approach to find neural patterns and their evolution over time in the signal.
On the group level, however, ... \\
In typical \gls{meg} analyses, group analyses are conducted by ... .
However, it is difficult to disentangle whether distinct neural activation patterns across individuals are due to idiosyncratic cognitive processing, or if different participants have differently encoded but shared functional states.
In \gls{fMRI}, functional alignment methods have become popular to discover shared representations in a high-dimensional functional space \citep{haxby2020hyperalignment}.
But as outlined in \ref{preprocessing}, \gls{tSSS} decomposes the neural signal from the 306 dimensional sensor space into an 80-dimensional subspace, the \gls{SSS} basis.
The signal of interest in \gls{meg} data can thus clearly be expressed in fewer than 306 dimensions.
\gls{SRM} \citep{NIPS2015_b3967a0e} is a method that combines functional alignment and dimensionality reduction.
In the following analyses, I therefore sought to find out whether I can find meaningful low-dimensional structure in the high-dimensional dataset by means of functional alignment via \gls{SRM}.\\
\gls{SRM} models each subject $i$'s response to temporally synchronized stimuli $X_i \in \mathbb{R}^{s \times t}$ as an orthogonal subject-specific base $W_i \in \mathbb{R}^{s\times k}$ and a $k$-dimensional shared response $S \in \mathbb{R}^{k \times t}$ such that


\begin{equation}
	\begin{aligned}
		min_{w_i, s}\sum_i{\|X_i - W_iS \|}^2_F \\
		s.t. W^T_iW_i = I_k
	\end{aligned}
	\label{eq:srm}
\end{equation}

where $t$ is the number of time points, $s$ is the number of sensors, $k$ is a hyper-parameter denoting dimensionality, and $\|\cdot\|_F$ denotes the Frobenius norm.
The orthonormal constraint $W^T_iW_i = I_k$ yields the computational advantages of robustness and preserving temporal geometry \citep{NIPS2015_b3967a0e}.
Unlike, for example, \gls{pca}, the reduced dimensionality does not only emerge from the data of a single brain, but taking the activation patterns of several participants into account.
It is a latent factor model, and rather than searching for direct correspondences across subjects, an initial dimensionality reduction is used to identify latent factors that are shared across subjects and support the observed measurements.
One latent feature $k$ is a functional topography, and the \gls{SRM} models the neural signal as a linear combination of subject-specific functional topographies (see Figure \ref{fig:srm-basics-multi}).\\
A central underlying assumption of functional alignment methods can be summarized as follows: When subjects experience the same events, their brain activity patterns may differ anatomically, but they should correspond to similar cognitive processes.
However, an explicit requirement is therefore temporal synchronicity of stimulation across participants.
For this reason, most applications of functional alignment use feature-rich, naturalistic stimulation such as movies \citep{haxby2020hyperalignment,bazeille2021empirical}.
Nevertheless, the cleaned epochs from the experiment can be brought into a standardized order to meet this assumption \citep{chentutorial}.
Given participants experience the same sequence of stimulus events, \gls{SRM} identifies common activity patterns across subjects, and provides a method to transform the original activity into a lower dimensional shared latent component space.
Where the original data in sensor space is a $306$ sensors $\times$ number of samples matrix, the shared response space is a $k$ features $\times$ number of samples matrix, and the subject specific weights that map between the shared space and each subject's idiosyncratic sensor space is an orthogonal matrix of dimensionality $306$ sensors $\times$ $k$ features.
After the \gls{SRM} is fit, several procedures become possible, such as mapping one participant's data to a different participant's functional topography, denoising neural data by transforming the shared response via the subject-specific basis back to the individual participant's original topography, or inferring idiosyncratic signal components in individual participants' data by subtracting the shared response \citep{NIPS2015_b3967a0e}.
The upcoming analyses showcase some of these and yet other use cases for the \gls{SRM}.\\


\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{memento/srm/srm_basics_3.png}
	\caption[SRM overview]{SRM overview: Sensor-by-time matrices are decomposed into a orthogonal subject specific basis matrix (sensor-by-features), a common shared feature matrix (features-by-time), and a subject-specific error}
	\label{fig:srm-basics-multi}
\end{figure}


%In a recent comparison of functional alignment methods, \gls{SRM} yielded the highest decoding accuracy gains on naturalistic \gls{fMRI} data \citep{bazeille2021empirical}.
%\citep{janati2020multi} employed functional alignment to perform source reconstruction.
%\gls{SRM} is successfully and widely used with fMRI data \citep{bazeille2021empirical}, in particular for naturalistic acquisitions with free-viewing tasks and movie stimuli (cite Sam).
%More rarely, the method is used in experimental paradigms the method is only rarely used in \gls{meg} or trial-based experimental designs.
%\citet{kalyani2023reduced} used shared response modeling to decode somatosensory finger maps, and changes in their representation with age.
%\citet{xie2021minimal} employed the method to functionally align intracranial EEG recordings.
%Thus the methods' utilities was first tested with a number of analyses intended to serve as sanity checks.

%\begin{figure}
%	\centering
%	\includegraphics[width=0.9\textwidth]{memento/srm/srm_basics_1.png}
%	\caption[SRM overview]{\gls{meg} data can be represented as a matrix. In \gls{SRM} on \gls{meg} data, sensor-by-time matrices are decomposed into a orthogonal subject specific basis matrix (sensor-by-features), a common shared feature matrix (features-by-time), and a subject-specific error}
%	\label{fig:srm-basics}
%\end{figure}

Initially: SRM on a per subject level, with each epoch treated as its own subject (loosen the definition of a subject). The result, individual trial weights, were used to transform trial data from sensor space to shared space, and thus cleaned from idiosyncratic elements.
I visualized the individual features, and calculated the correlation distance between timepoints in the shared space.


\subsubsection{Sanity checks}

As \gls{SRM} has rarely been employed in \gls{meg} data in the published literature, I first investigated whether basic stimulus properties are retained in the lower-dimensional shared response space of the model.
This can serve as a sanity check if information of interest is retained.
For this, I used the nine different probability and magnitude combinations that were used for the left stimulus option (letters A-I in Figure \ref{fig:memento_stim}) as stimulus identities of interest to inform trial ordering and detect condition differences.
I used two different approaches to visualize the shared space for this:
One based on correlation distances of features in the shared space, to detect dissimilarities across features.
And one based on using the shared space to either visualize its features over a stereotypical trial course or transform unseen test data into shared space.
The
Prior to training the shared response model, I preprocessed the neural data as follows:
(Train-test split, Reordering, averaging, SRM fitting)
I trained probabilistic shared response model on a training set of neural data, ordered by
We normalized epoch data by z-scoring each epoch within sensors.
For this, a shared response model was trained on a subset of data.
Next, we calculated the correlation distance between the shared response during one type of trial to al other trial types, for both the left and the right stimulus option.
The largest correlation distances, however, did not emerge between different stimulus identities, but between stimuli presented on the left versus the right side of the screen (Figure TODO).
An underlying assumption for this is that neural signals differ across stimulus types.


\subsection{Shared response modeling in spectral space}

As previously introduced, shared response modeling relies on the assumption that different participants display similar cognitive processes in response to the same tasks or stimuli \citep{NIPS2015_b3967a0e}.
In previous analysis, I generated artificial correspondence of first options by reordering the trial sequence.
But this reordering only concerned the visual stimulation.
The distribution of reaction times in the task (\ref{behavioral-analysis}), however, revealed different processing speeds, evident in intra- and inter-individual variation in response time.
Given previous findings ... Similar variation in cognitive processing could exist during working memory maintenance during the delay.
Thus, the activation of interest could be invisible for the shared response model if it occurs at too different time points.
To use the shared response model despite idiosyncratic processing, I transformed the input data into a space without timing information.
For this, I drew inspiration from spectral analysis.\\
Spectral analysis consists of deconstructing a time domain signal of a given length into its constituent oscillatory components using Fourier analysis.
MEG signal consists of oscillations with an amplitude, a frequency, and a phase.
Frequency, usually measured in hertz (Hz), is the number of times a specified event occurs within a specified time interval, while amplitude is the height, force or power of the wave, and power is the squared amplitude.
Phase involves the relationship between two or more signals that share the same frequency and describes the relationship between the position of the amplitude crests and troughs of two waveforms, measured in distance, time, or degrees.
If the peaks of two signals with the same frequency are in exact alignment at the same time, they are said to be in phase and out of phase otherwise.
If the exact timing or duration of shared activity in question is subject to intra- and inter-individual variation, it is not necessarily phase-locked.
However, a spectral decomposition of this signal results in power spectrum, with the frequency domain of the oscillations on the x axis, and the amplitude on the y axis.
Thus, the signal is expressed as a function of frequency rather than time.
Such a transformation from a time-resolved representation of data to a representation in the frequency domain can be used to find shared signals with different temporal signatures [CITE probably every introductory MEG book].
Importantly, the model bases of an \gls{SRM} assign weights to sensors regardless of whether they contribute to a shared signal in time-resolved or spectral space.
Therefore, we can fit a shared response model on spectral data to find shared frequency components, but then use the model basis to transform time-resolved data (see Figure \ref{fig:spectral-srm}).
This not only allows the visualization of shared components as a time series, but also easier interpretation of components in the context of the experimental paradigm.

Advantage of spectral transformation: No need to decide for a frequency band(?) given the unclear evidence across studies.

\begin{figure}
	\centering
	\begin{subfigure}{0.9\textwidth}
		\includegraphics[width=\textwidth]{memento/spectral_srm/spectral_srm_basics_1.png}
		\caption{Fitting the shared response model on spectral data}
		\label{fig:spectral-srm1}
	\end{subfigure}
	\begin{subfigure}{0.9\textwidth}
		\includegraphics[width=\textwidth]{memento/spectral_srm/spectral_srm_basics_2.png}
		\caption{Transforming time resolved data into the shared space}
		\label{fig:spectral-srm2}
	\end{subfigure}
	\caption[Spectral shared response modeling]{Spectral shared response modeling: A shared response model decomposes spectral neural data (\ref{fig:spectral-srm1}, green) into a shared spectral space (features by frequencies; orange) and subject-specific basis (sensors by features; yellow). Because the subject specific bases transform sensor data independent on whether it is time resolved or spectral, they can be used to transform time resolved data (\ref{fig:spectral-srm2}; green) into time courses of the shared response spaces features. Capital ``T'' denotes a matrix transpose.}
	\label{fig:spectral-srm}
\end{figure}

\subsection{Simulation study}


\begin{figure}
	\begin{subfigure}{1.0\textwidth}
		\includegraphics[width=.5\textwidth]{memento/simulation/sim_0.png}
		\includegraphics[width=.46\textwidth]{memento/simulation/sim_10.png}
	\end{subfigure}
	\caption{Artificial signal for simulation]}{An artificial signal with 10.000 samples and a 1000 sample spanning 10Hz main frequency component, pure (left) or partially embedded into 306 sensors with Gaussian noise (right). Roughly 20 percent of sensors contain the main signal in varying amounts, mirroring how brain signals are only present in certain sensors and decrease in strength over distance.}
	\label{fig:sim_artificial_signal}
\end{figure}

I investigated the feasibility of this method with a simulation study:
For this, I generated a ``ground truth'' signal with a given frequency and wave form, and embedded it partially into 306 artificial sensors with Gaussian noise in order to simulate \gls{meg} data (Figure \ref{fig:sim_artificial_signal}).
To mirror how brain signals are only present in certain sensors and with varying strengths, only a fixed amount of sensors receives the signal.
For each sensor with a signal, a random weight between is drawn that determines the strength with which the signal is scaled.
This is repeated for N artificial subjects, but with a different random phase offset in the signal for each to simulate data from several subjects with a common frequency component that occurs at different points in time.
If the \gls{SRM} is successful, the shared response should reflect the ground truth signal despite phase offsets, and the subject-specific model bases should reflect the magnitudes of the weights for each sensor.\\
Using this artificial data as the basis for shared response modeling, I fitted a probabilistic \gls{SRM} with $k=10$ features to recover the signal as a shared component - either on time resolved data (Figure \ref{fig:sim-timeresolved}), or after transforming the data into its frequency spectrum (Figure \ref{fig:sim-spectral}).\\
When fit on time resolved data with phase shifts, the resulting shared space consists of components that differentially picked up signals from one or more participants, but represent it in a time series of repeated or overlapping signals.
This makes an interpretation of individual components difficult (Figure \ref{fig:sim-timeresolved-shared}).
The sensor weights that the model estimates for each component also do not show a clear association with the true weights used in the generation of the artificial data (Figure \ref{fig:sim-timeresolved-weights}).
In other words, with phase shifts between individual simulations' signals, different components of the shared space capture several participants' signals, but no \textit{general} ground truth signal.
However, transforming the signal into its frequency components, the spectral space, removes timing information, and with it, the phase offsets.
While the components in spectral space are not easy to interpret or differentiate (see Figure \ref{fig:sim-spectral-shared}), the scatterplot reveals that certain components' weights show a clear association to the weights used for model generation \ref{fig:sim-spectral-weights}.\\
This becomes apparent when shared components are visualized as time series by using the subject-specific weight matrices of the \gls{SRM} and individual subject's time-resolved data.
One or few components are consistently representing the original signal well across subjects, laying the basis for both identifying shared signal in phase-shifted data and interpreting the shared components resulting from it.


\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{memento/simulation/sim_4.png}
		\caption{SRM on time-resolved data}
		\label{fig:sim-timeresolved-weights}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{memento/simulation/sim_6.png}
	\caption{SRM on spectral data}
	\label{fig:sim-spectral-weights}
	\end{subfigure}
	\caption[Relationship of model weights and true weights]{Recovered versus true model weights from an \gls{SRM} fitted on time resolved data with phase shifts (left) or fitted on spectrally transformed data with phase shifts (right): Without spectral transformation, the relationship between model weights and ground truth weights appears mostly random. When fit on spectrally transformed data, the relationship between model weights and ground truth weights clearly captures an association for some of the $k=10$ components, indicating successful recovery the original weights.}
	\label{fig:sim-weights}
\end{figure}


\begin{figure}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=\textwidth]{memento/simulation/sim_3.png}
		\caption{Shared space from time-resolved data}
		\label{fig:sim-timeresolved-shared}
	\end{subfigure}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=\textwidth]{memento/simulation/sim_5.png}
		\caption{Shared space from spectral data}
		\label{fig:sim-spectral-shared}
	\end{subfigure}
	\hfill
	\begin{subfigure}{1.\textwidth}
		\includegraphics[width=.499\textwidth]{memento/simulation/sim_7.png}
		\includegraphics[width=.499\textwidth]{memento/simulation/sim_8.png}
		\caption{Shared components from \ref{fig:sim-spectral-shared}, visualized by transforming time resolved data of two different subjects}
		\label{fig:sim-shared-transformed}
	\end{subfigure}
	\caption[Properties of a shared time-resolved or spectral space]{Properties of a shared time-resolved or spectral space. a) visualizes the shared space resulting from a model fit on time resolved data. The Gaussian noise is evidently reduced while signal of interest is retained. However, it shows that the signals from several subjects as consecutive or overlapping activity. Using individual subject's weight matrices and raw data to visualize the individual component model compontents reveal that different components capture the signal of interest across subjects (TODO). b) shows the shared spectral space, and c) and d) the components for two different subjects. The components in shared space are spectral and difficult to interpret. But when time-resolved data from two different subjects is transformed using the subject specific bases from the \gls{SRM}, the individual subject's offset re-emerges. Importantly, the strength with which components capture the signal of interest is consistent across subjects. Although some components capture the signal of interest partially, the first component shows it most strongly and across participants.}
	\label{fig:sim-spectral}
\end{figure}




\subsection{Shared response modeling in spectral space on real data}

After the simulation study demonstrated principal feasibility of the method, I subsequently applied it to the neural recordings from the memento study.
The underlying aim was to investigate how decision-relevant properties are represented in working memory throughout the delay phase.
This was done with several exploratory analysis that followed the same general principle:
In a first step, data was split into a training and a test set.
For this, epochs were shuffled and randomly drawn to not introduce experiment fatigue as a confound.
A probabilistic shared response model was then fit on the spectrally transformed training data, yielding a shared spectral space (features by frequencies) and subject-specific basis (sensors by features) (\ref{fig:spectral-srm1}).
Importantly, the model had never seen the test data.
Using the subject-specific bases, the time-resolved test data was then transformed in the shared space while retaining the temporal resolution (\ref{fig:spectral-srm2}) to visualize the components the shared response model found.

\subsection{Decoding}
\label{decoding-analysis}

Finally, I performed a last attempt to find stimulus features in the signal during the delay with a temporal decoding analysis.
This analysis involves fitting a multivariate predictive model on each time instant within a trial, and evaluating the model performance at the same instant on new epochs.
It is similar to a searchlight analysis (CITE KRIEGESKORTE).
Other than implementing a standard temporal decoding pipeline, I also implemented custom pipelines that employed internal dimensionality reduction.

The pipeline allowed to set the following parameters to improve decodability:
\begin{itemize}
	\item Bootstrapping:
	\item Trial averaging:
	\item Sliding window
	\item Dimensionality k (for PCA, SRM, and spectral SRM)
	\item Number of timeseries used in training (for SRM and spectral SRM)
\end{itemize}

Prior to decoding neural signals during the delay phase, I used data from the motor response of the trial to optimize decoding parameters of the analysis.
I based the evaluation on two measures, visualized in Figure \ref{fig:opti-eval}

\begin{figure}
	\includegraphics[width=.6\textwidth]{memento/decoding/decoding_eval.png}
	\caption[Decoding evaluation]{Two measures served for model evaluation: The average accuracy 500ms prior to the motor response, and the peak accuracy in this time frame.}
	\label{fig:opti-eval}
\end{figure}

%\begin{figure}
%	\includegraphics[width=.6\textwidth]{}
%	\caption[]{}
%	\label{}
%\end{figure}

Due to the high temporal resolution of \gls{meg}, fitting models on data with 1000Hz would be computationally intensive, but also noisy.
Thus, it is common to apply sliding windows to the data.
I implemented two separate sliding windows: A temporal sliding window, integrating signal in the time dimension.
And a spatiotemporal sliding window, integrating information over both space and time.

\begin{figure}
	\begin{subfigure}{0.6\textwidth}
		\includegraphics[width=\textwidth]{memento/decoding/sliding_window.png}
		\caption{Temporal sliding window}
		\label{fig:temporal-slider}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth]{memento/decoding/spatiotemporal_sliding_window.png}
		\caption{Spatiotemporal sliding window}
		\label{fig:spatiotemporal-slider}
	\end{subfigure}
	\caption[Schematic of different sliding window types]{Schematic of different sliding window types. \ref{fig:temporal-slider} depicts a temporal sliding window, integrating the signal across samples via averaging. \ref{fig:spatiotemporal-slider} depicts a spatial sliding window. All samples in the sliding window are concatenated such that the one resulting sample per sliding window includes the spatiotemporal configuration of all sensors in the window}
	\label{fig:sliding-windows}
\end{figure}

Scikit-learn expects two-dimensional data.
Epoched \gls{meg} data, however, is at least three-dimensional: Trials $\times$ channels $\times$ trial-length.
To solve this issue, \textit{mne-python} uses so-called \textit{vectorizers} that transposes three-dimensional data into the desired two-dimensional format.

%Optimizing decoding parameters - Sliding windows:
In the spatiotemporal slider, the data entering the decoders were pooled over multiple time points, so as to exploit not only information that is encoded in spatial activation patterns, but also information that is encoded in their temporal structure, similar to \citep{muhle2021hierarchy}.
% TODO: illustrations of the sliders


% visualizing compontents
\citet{murray2017stable} projected neural trajectories into a mnemonic subspace spanned by individual components from PCA.

%decoding with dimensionality reduction
\citet{murray2017stable} decoded stimulus representations from lower-dimensional subspaces.

\pagebreak



\section{Discussion}

%RDM discussion

The work I put into the preparation of the dataset has made my own data analyses possible, and further analyses by future researchers feasible.
The idiosyncratic structure of the raw dataset has posed a significant challenge to identify relevant files, distinguish processing states, understand the dataset as a whole, and base processing on it.
The conversion to the \gls{BIDS} standard for \gls{meg} was an indispensable prerequisite to working with this data.


%Why has shit not worked.
% One option: The signal of interest is not dominant enough.
It has been demonstrated that the data after \gls{SSS} represents the most dominating magnetic field patterns in \gls{meg} recordings \citep{garces2017choice}.


For the past decades, scientific publishing has favored significant over nonsignificant findings \citep{dwan2008systematic}, a condition termed \textit{publication bias} or \textit{file drawer problem} \citep{rosenthal1979file}.





Low statistical power reduces not only the likelihood to find a true effect, but also the likelihood that statistically significant results reflect a true effect \citep{button2013power}.
And as increased noise in small samples inflates the effect size of significant results \citep{loken2017measurement}, published low-powered studies likely overestimate the effects that they report.
In a systematic review by \citet{pavlov2022oscillatory}, the average number of participants in experimental studies of verbal or visual working memory was $N_{verbal}=19.3$ and $N_{visual}=23.3$, respectively.

However, \citet{pavlov2022oscillatory} highlighted the lack of agreement in the field in a systematic review recently, and pointed to various confounding factors in past studies. They outlined, for example, that gamma band oscillations found in \gls{eeg} studies are more likely to be muscle components.
This uncertainty makes it difficult to hypothesize.
