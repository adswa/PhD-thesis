% !TeX root = ../main-english.tex
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% -*- coding:utf-8 mod:LaTeX -*-

%This smart spell only works if no changes have been made to the chapter
%using the options proposed in preambel/chapterheads.tex.
\setchapterpreamble[u]{%
	\dictum[Isaac Newton]{If I have seen further it is by standing on the shoulders of Giants}
}

\chapter{Discussion}
\label{discussion}

The original aim of this work was to find novel insights about brain state transitions in a delayed decision making task from previously unpublished data.
However, the context in which this project was conducted made research data management and research software engineering particularly central.
Viewed in conjunction, this thesis has been a testament to the foundational importance that organizational and technical aspects of scientific conduct play in research endeavors.
Chapter \ref{chap:k1} introduced preexisting tools and solutions for managing research data and projects.
The \gls{FAIR} principles and the \gls{BIDS} standard are established elements of good \gls{rdm}, and DataLad a promising software tool for data versioning, transport, and digital provenance capture.
In conjunction, they enable researchers to create scientific outputs that are portable and reusable as stand-alone, well-described units, without reliance on the original authors - and thus, a fitting tool set to conduct the planned analysis.
However, Chapter \ref{chap:k1} also shed light on DataLad's shortcomings and deficiencies.
Thus, my work on the DataLad Handbook, outlined in Chapter \ref{chap:k2} and rooted in the area of research software engineering, improved some of these with documentation and workflows.
Continuing in a similar spirit, Chapter \ref{chap:k3} outlined the practical difficulties of creating reusable research outputs that arise despite the existence of the \gls{FAIR} principles because full FAIRness can not always be achieved.
To address them, I conceived a set of four pragmatic research data management strategies that can make research objects more reusable, even when they are not yet fully \gls{FAIR}.
Chapter \ref{chap:k3} then concluded with a computational framework that put these strategies into practice, and highlighted our proof of principle work to test its applicability and scalability on a neuroscientific dataset of the largest scale.
Chapter \ref{chap:k4} then put the technical and organizational work of previous chapters into action.


of research data management and research software engineering in scientific work.


Look mom, some text!



File content transport across this network is possible via versatile transport logistics that allow for local or remote data hosting. This can enable data transports on systems with too little available disk space for multiple copies, allow redundant storage to be configured, interoperate with hosting services to publish results, or reconfigure data access when remote hosting locations change—without needing to alter the data representation in the dataset.

With these technical features, how and where data are stored (e.g., local, encrypted storage; remote, cloud-based hosting) becomes orthogonal to how and where computations are performed (e.g., on-site compute cluster; remote cloud-computing service). This allows our framework to bootstrap ephemeral (short-lived) workspaces for individual computational jobs, retrieve only relevant processing elements (e.g., subsets of input data), and extend the DataLad datasets’ revision history with their results and process provenance (Figure 1c). This, in turn, opens the possibility for parallel and version controlled analysis progression, using a distributed network of temporary clones. Results and revision histories can be merged to form a full processing history, in a similar way to how code is collaboratively developed with distributed version control tools




The modular assembly enables (re)use of independently maintained components, separates access modalities such that access-restricted input data does not impair sharing of less sensitive outcomes, and guarantees precise identification of processing components, regardless of whether a particular dataset consumer has access to a given component.




\section{Conclusion}

\section{Limitations}


\section{Opportunities}


